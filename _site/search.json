[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Advanced Psychological Statistics",
    "section": "",
    "text": "Welcome to the first day of class! If you want to follow along with examples, head over to github for all of the code and data. https://github.com/josh-jackson/glm_2025"
  },
  {
    "objectID": "12-ml.html#today-last-time",
    "href": "12-ml.html#today-last-time",
    "title": "ML-2",
    "section": "Today & Last time‚Ä¶",
    "text": "Today & Last time‚Ä¶\n\nResampling methods via bootstrapping & permutation testing\nLessons from machine learning"
  },
  {
    "objectID": "12-ml.html#explanation-vs.-prediction",
    "href": "12-ml.html#explanation-vs.-prediction",
    "title": "ML-2",
    "section": "Explanation vs.¬†Prediction",
    "text": "Explanation vs.¬†Prediction\nExplanation: describe causal underpinnings of behaviors/outcomes\nPrediction: Accurately forecast behaviors/outcomes\nSimilarities.\n\nBoth are goals of science.\nGood predictions can help us develop theories of explanation and vice versa"
  },
  {
    "objectID": "12-ml.html#explanation-vs.-prediction-1",
    "href": "12-ml.html#explanation-vs.-prediction-1",
    "title": "ML-2",
    "section": "Explanation vs.¬†Prediction",
    "text": "Explanation vs.¬†Prediction\nStatistical Tensions\n\nStatistical models that accurately describe causal truths often have poor prediction and are complex\nPredictive models are often very different from the ‚Äútrue‚Äù, underlying data generating processes"
  },
  {
    "objectID": "12-ml.html#explanation-vs.-prediction-2",
    "href": "12-ml.html#explanation-vs.-prediction-2",
    "title": "ML-2",
    "section": "Explanation vs.¬†Prediction",
    "text": "Explanation vs.¬†Prediction\n\nWhat do we do in Psychology and Neuroscience?\nWhat about regression?\n\nPoint of Yarkoni & Westfall (2017) - How far have we really come in the past 10 years? 20 years? 30 years? ‚Ä¶not as far as we‚Äôd like\n\nWe should spend more time and resources developing predictive models than we currently do"
  },
  {
    "objectID": "12-ml.html#maybe-helpful-definitions",
    "href": "12-ml.html#maybe-helpful-definitions",
    "title": "ML-2",
    "section": "Maybe Helpful Definitions",
    "text": "Maybe Helpful Definitions\nMachine Learning (ML) is the process of feeding previous observations into a computer and using the computer to generate predictions for new observations. AKA:\n\nmachine inference\npattern recognition (Google Photos!)\nstatistical estimation\nprediction modeling\nstatistical learning (ish)"
  },
  {
    "objectID": "12-ml.html#maybe-helpful-definitions-1",
    "href": "12-ml.html#maybe-helpful-definitions-1",
    "title": "ML-2",
    "section": "Maybe Helpful Definitions",
    "text": "Maybe Helpful Definitions\nTraining occurs by extracting patterns from the observed data; think of this as learning\nTesting occurs by verifying predictions on previously unobserved data; think of this as evaluating\nArtificial Intelligence is not the same as machine learning. AI is the simulation of human intelligence by computers; AI systems are generally trained with machine learning approaches."
  },
  {
    "objectID": "12-ml.html#some-more-definitions",
    "href": "12-ml.html#some-more-definitions",
    "title": "ML-2",
    "section": "Some more definitions",
    "text": "Some more definitions\nSupervised Learning: using known patterns between input and output observations to train a mapping between the two\n\nRegression! learning the mapping between a continuous input feature variable and a continuous output target variable\nClassification: learning the mapping between a continuous input feature variable and a categorical output target variable (i.e., a label)"
  },
  {
    "objectID": "12-ml.html#some-more-definitions-1",
    "href": "12-ml.html#some-more-definitions-1",
    "title": "ML-2",
    "section": "Some more definitions",
    "text": "Some more definitions\nUnsupervised Learning: determining patterns in observations without guiding referents\n\nDimensionality Reduction: decreasing the overall number of features considered in a learning procedure (i.e., PCA, ICA etc.)\nClustering: grouping features together that are similar as determined by some metric"
  },
  {
    "objectID": "12-ml.html#wash-u",
    "href": "12-ml.html#wash-u",
    "title": "ML-2",
    "section": "@Wash U",
    "text": "@Wash U\n\nReinforcement Learning: determining a mapping between input and output observations using only a measure of training quality\nML classes in CS department\nWouter Kool for reinforcement learning\nACCSN with Dennis Barbour and myself for high level discussions on these topics\nPoli Sci department for dealing with categorical outcomes"
  },
  {
    "objectID": "12-ml.html#machine-learning-algorithms",
    "href": "12-ml.html#machine-learning-algorithms",
    "title": "ML-2",
    "section": "Machine Learning Algorithms",
    "text": "Machine Learning Algorithms\n\n\n\nordinary least squares linear regression\nlogistic regression\nk-means clustering\nnearest neighbor\nnaive Bayes\nridge regression\nLASSO regression\nsupport vector machine\n\n\n\nrandom forest\nGaussian process estimator\nmultilayer perceptron (deep net)\nconvolutional network\nrecurrent network\ngeneralized adversarial network"
  },
  {
    "objectID": "12-ml.html#some-terminology",
    "href": "12-ml.html#some-terminology",
    "title": "ML-2",
    "section": "Some terminology",
    "text": "Some terminology\nOverfitting is when we mistakenly fit sample-specific noise as if it were actually a signal.\n\nIf our model has a \\(R^2 = .9\\), we do an excellent job of explaining variance in our sample.\nOLS models tend to be overfit because they minimize error for a specific sample"
  },
  {
    "objectID": "12-ml.html#bias-and-variance",
    "href": "12-ml.html#bias-and-variance",
    "title": "ML-2",
    "section": "Bias and Variance",
    "text": "Bias and Variance\nBias refers to systematically over- or under-estimating parameters.\nVariance refers to how much estimates tend to jump around\nBias-Variance Tradeoff we can reduce variance around our estimates but at the expense of increasing bias of estimates and vice versa\nUnderfitting means we can‚Äôt capture a relationship at all ‚Äì not as big of a problem for us"
  },
  {
    "objectID": "12-ml.html#cross-validation",
    "href": "12-ml.html#cross-validation",
    "title": "ML-2",
    "section": "Cross-validation",
    "text": "Cross-validation\nCross-validation is a family of techniques that involve testing and training a model on different samples of data."
  },
  {
    "objectID": "12-ml.html#cross-validation-hold-out-samples",
    "href": "12-ml.html#cross-validation-hold-out-samples",
    "title": "ML-2",
    "section": "Cross-validation: Hold-out Samples",
    "text": "Cross-validation: Hold-out Samples\n\n\n\nSplit into training and testing sets\nFit your model on the training set\nPredict outputs for your testing set\n\n\nPros\n\nStraightforward; computationally easy\n\nCons\n\nWhich data go into which set?\nWhat if the vast majority of group A fall into the training set and the vast majority of group B fall into the testing set?"
  },
  {
    "objectID": "12-ml.html#k-fold-cross-validation",
    "href": "12-ml.html#k-fold-cross-validation",
    "title": "ML-2",
    "section": "K-fold Cross-validation",
    "text": "K-fold Cross-validation\n\nMake k subsets of your data\nRepeat the hold-out method of test/train, but do it k times\nGet the model fit for all k iterations; take the average model fit"
  },
  {
    "objectID": "12-ml.html#k-fold-cross-validation-1",
    "href": "12-ml.html#k-fold-cross-validation-1",
    "title": "ML-2",
    "section": "K-fold Cross-validation",
    "text": "K-fold Cross-validation\nPros\n\nDoesn‚Äôt matter much which data points fall into test or train since each subset can be both a test and a training set\nThe more folds you do (larger k), the more you are able to decrease your variance around your averaged model fit\n\nCons\n\nCan take a decent amount of computational power, depending on the dataset"
  },
  {
    "objectID": "12-ml.html#leave-one-out-cross-validation",
    "href": "12-ml.html#leave-one-out-cross-validation",
    "title": "ML-2",
    "section": "Leave-One-Out Cross-validation",
    "text": "Leave-One-Out Cross-validation\n\nSame as k-fold, but now k is equal to your \\(N\\)\n\nPros\n\nGood estimations\n\nCons\n\nEven more computationally expensive\nEspecially if using ‚Äúbig data‚Äù"
  },
  {
    "objectID": "12-ml.html#a-brief-aside",
    "href": "12-ml.html#a-brief-aside",
    "title": "ML-2",
    "section": "A brief aside:",
    "text": "A brief aside:\nNewer package called tidymodels is better for machine learning, but requires many more steps. For now, this is the simpler method.\nR vs.¬†Python üêç . R is excellent for statistics and visualizing data. Most of what we do in Psychology. Python is better for machine learning and is more of a full suite language.\nIf you‚Äôre going hard with ML, use tidymodels or Python."
  },
  {
    "objectID": "12-ml.html#example-10-fold-cross-validation",
    "href": "12-ml.html#example-10-fold-cross-validation",
    "title": "ML-2",
    "section": "Example: 10-fold cross validation",
    "text": "Example: 10-fold cross validation\n\n\nCode\nlibrary(caret)\n# set control parameters\nctrl &lt;- trainControl(method=\"cv\", number=10)\n# use train() instead of lm()\ncv.model &lt;- train(Stress ~ Anxiety*Support*group, \n               data = stress.data, \n               trControl=ctrl, # what are the control parameters\n               method=\"lm\") # what kind of model\ncv.model\n\n\nLinear Regression \n\n118 samples\n  3 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 106, 106, 106, 106, 106, 107, ... \nResampling results:\n\n  RMSE      Rsquared   MAE     \n  1.541479  0.3708104  1.248871\n\nTuning parameter 'intercept' was held constant at a value of TRUE"
  },
  {
    "objectID": "12-ml.html#cross-validation-summary",
    "href": "12-ml.html#cross-validation-summary",
    "title": "ML-2",
    "section": "Cross-Validation Summary",
    "text": "Cross-Validation Summary\n\nTempers your estimates to protect against overfitting\nYou do not need fancy ML algorithms. You can do this with your classic regression!\nIf your whole sample is not representative, this will not save you"
  },
  {
    "objectID": "12-ml.html#regularization",
    "href": "12-ml.html#regularization",
    "title": "ML-2",
    "section": "Regularization",
    "text": "Regularization\nPenalizing a model as it grows more complex.\n\nUsually involves shrinking coefficient estimates ‚Äì the model will fit less well in-sample but may be more predictive"
  },
  {
    "objectID": "12-ml.html#lasso-regression",
    "href": "12-ml.html#lasso-regression",
    "title": "ML-2",
    "section": "LASSO Regression",
    "text": "LASSO Regression\n\nThe glmnet package has the tools for LASSO regression.\nOne small complication is that the package uses matrix algebra, so you need to feed it a matrix of predictors ‚Äì specifically, instead of saying ‚Äúfind the interaction between A and B‚Äù, you need to create the variable that represents this term.\nLuckily, the function model.matrix() can do this for you."
  },
  {
    "objectID": "12-ml.html#what-value-of-lambda-to-choose",
    "href": "12-ml.html#what-value-of-lambda-to-choose",
    "title": "ML-2",
    "section": "What value of \\(\\lambda\\) to choose?",
    "text": "What value of \\(\\lambda\\) to choose?\nLooks like coefficients 1, 2, and 3 have high values even with shrinkage.\n\n\nCode\nplot(lasso.mod, xvar = \"dev\", label = T, )"
  },
  {
    "objectID": "12-ml.html#what-value-of-lambda-to-choose-1",
    "href": "12-ml.html#what-value-of-lambda-to-choose-1",
    "title": "ML-2",
    "section": "What value of \\(\\lambda\\) to choose?",
    "text": "What value of \\(\\lambda\\) to choose?\nLooking for \\(\\lambda\\) values where those coefficients are still different from 0.\n\n\nCode\nplot(lasso.mod, xvar = \"lambda\", label = TRUE, cex = 8)"
  },
  {
    "objectID": "12-ml.html#summary-yarkoni-and-westfall-2017",
    "href": "12-ml.html#summary-yarkoni-and-westfall-2017",
    "title": "ML-2",
    "section": "Summary: Yarkoni and Westfall (2017)",
    "text": "Summary: Yarkoni and Westfall (2017)\nBig Data * Reduce the likelihood of overfitting ‚Äì more data means less error\nCross-validation * Is my model overfit?\nRegularization * Constrain the model to be less overfit"
  },
  {
    "objectID": "2-HW.html",
    "href": "2-HW.html",
    "title": "Homework 2",
    "section": "",
    "text": "Data: You will need to use the Dawtry et al., 2015 Psychological Science article and the accompanying dataset to answer all questions. Please download this dawtry-hw2-data dataset for the homework, which corresponds to study 1a of the paper. The paper can be found here.\nRemember that if you work with a peer or use AI tools like Chat-GPT, you must cite them in your code.\nYou will submit both your .Rmd (or .qmd) and HTML files to Canvas. For all figures, you must be intentional. Change the axes names, the theme etc. Do not settle for the defaults!\nGood luck!"
  },
  {
    "objectID": "2-HW.html#question-1",
    "href": "2-HW.html#question-1",
    "title": "Homework 2",
    "section": "Question 1",
    "text": "Question 1\nGraph the distributions of the following items. Be sure to comment on anything you find irregular or interesting. The variables to be plotted are:\n\nHousehold Income\nFairness and Satisfaction\nSocial Circle Mean Income\nPopulation Mean Income\nPolitical Preference"
  },
  {
    "objectID": "2-HW.html#question-2",
    "href": "2-HW.html#question-2",
    "title": "Homework 2",
    "section": "Question 2",
    "text": "Question 2\nRun a regression where political preferences predict redistribution:\n\nWrite the formal equation (be specific!)\nInterpret the intercept and the regression coefficient (be specific!)\nStandardize the variables, rerun the model, write the formal equation (be specific!) and interpret the intercept and the regression coefficient (be specific!)\nCalculate the correlation of the two variables (use the original variables in raw units). How does this relate to the standardized and unstandardized regression coefficients?\nCreated a fitted vs.¬†residuals plot. Do not use the defaults ‚Äì you must (at the very least) change the labels, colors, and give it a titles. Then interpret this plot."
  },
  {
    "objectID": "11-ml.html",
    "href": "11-ml.html",
    "title": "bootstrapping",
    "section": "",
    "text": "Start looking at topics related to machine learning\nWe are going to start off with Resampling methods"
  },
  {
    "objectID": "11-ml.html#today",
    "href": "11-ml.html#today",
    "title": "bootstrapping",
    "section": "",
    "text": "Start looking at topics related to machine learning\nWe are going to start off with Resampling methods"
  },
  {
    "objectID": "11-ml.html#first.-sampling",
    "href": "11-ml.html#first.-sampling",
    "title": "bootstrapping",
    "section": "First. Sampling",
    "text": "First. Sampling\nIt is impossible to survey every person in the population we are interested in, so we often take a ‚Äúrandom sample‚Äù from the population and calculate a sample statistic (e.g., mean, median).\nA lot of our statistics follow well-defined distributions (e.g., normal distribution), and we use the properties of these distributions to estimate the population parameter."
  },
  {
    "objectID": "11-ml.html#problems-with-sampling",
    "href": "11-ml.html#problems-with-sampling",
    "title": "bootstrapping",
    "section": "Problems with Sampling",
    "text": "Problems with Sampling\nSingle Estimate of the Population Parameter\n\nEstimate the population mean\nSampling distribution of mean derived from sample statistics\nBuilt a 95% confidence interval around the sample mean\nWe‚Äôve been relying on a single estimate of the population parameter (except for with Bayesian methods).\n\n\nWe make assumptions about the sample (e.g., representative sample, large sample size, normality), which may or may not be true."
  },
  {
    "objectID": "11-ml.html#introduction-to-resampling",
    "href": "11-ml.html#introduction-to-resampling",
    "title": "bootstrapping",
    "section": "Introduction to Resampling",
    "text": "Introduction to Resampling\nA statistical technique that involves re-estimation of the population parameter by repeatedly drawing samples from the original sample"
  },
  {
    "objectID": "11-ml.html#reasons-for-resampling",
    "href": "11-ml.html#reasons-for-resampling",
    "title": "bootstrapping",
    "section": "Reasons for Resampling",
    "text": "Reasons for Resampling\n\nReduce bias of the estimate by using multiple samples instead of one\nBetter sense of precision of the estimated population parameter\nWe do not need to make assumptions about the population distribution (e.g., when we perform two samples t-test, for example, we make the assumption that the populations from which the samples are drawn are normally distributed)\nSample does not have to be large"
  },
  {
    "objectID": "11-ml.html#types-of-resampling",
    "href": "11-ml.html#types-of-resampling",
    "title": "bootstrapping",
    "section": "Types of Resampling",
    "text": "Types of Resampling\n\nBootstrapping\nJackknife method (just a glimpse)\nPermutation testing (just a glimpse)\n\n\npull yourself up by your bootstraps\nImprove yourself/overcome (often impossible) obstacles without aid or assistance from anyone else. Use your own resources to improve your standing."
  },
  {
    "objectID": "11-ml.html#bootstrapping",
    "href": "11-ml.html#bootstrapping",
    "title": "bootstrapping",
    "section": "Bootstrapping",
    "text": "Bootstrapping\nBootstrapping is a method where we rely entirely on the sample that we have at hand.\n\nWe randomly sample within the sample (with replacement)\nCompute the estimator of interest in order to‚Ä¶\nBuild an empirical distribution of that test statistic."
  },
  {
    "objectID": "11-ml.html#illustration-of-bootstrapping",
    "href": "11-ml.html#illustration-of-bootstrapping",
    "title": "bootstrapping",
    "section": "Illustration of Bootstrapping",
    "text": "Illustration of Bootstrapping\nImagine we are trying to estimate the height of a cohort of friends. But you only have 6. Specifically, these 6 friends.\n\n\nTo estimate their height, you decide to perform bootstrapping, meaning you draw many samples from this group of 6 people, with replacement, each time calculating the average height of the sample.\n\n\nCode\nfriends = c('Monica', 'Rachel', 'Ross', 'Joey', 'Phoebe', 'Chandler')\nheights = c(165, 165, 178, 170, 172, 173)\nnames(heights) = friends\n\n(sample1 = sample(friends, size = 6, replace = T)); paste('Mean height of this sample:', mean(heights[sample1]))\n\n\n[1] \"Monica\"   \"Joey\"     \"Phoebe\"   \"Ross\"     \"Joey\"     \"Chandler\"\n\n\n[1] \"Mean height of this sample: 171.333333333333\"\n\n\n\nThe number of friends that we randomly sample is 6, the same number of friends inside the initial sample.\nWithin the same sample, there can be duplicates. This is what it means to randomly sample with replacement."
  },
  {
    "objectID": "11-ml.html#repeat",
    "href": "11-ml.html#repeat",
    "title": "bootstrapping",
    "section": "Repeat",
    "text": "Repeat\n\n\nCode\n(sample1 = sample(friends, size = 6, replace = T)); paste('Mean height of this sample:', mean(heights[sample1]))\n\n\n[1] \"Rachel\"   \"Joey\"     \"Joey\"     \"Phoebe\"   \"Monica\"   \"Chandler\"\n\n\n[1] \"Mean height of this sample: 169.166666666667\"\n\n\n\n\nCode\n(sample1 = sample(friends, size = 6, replace = T)); paste('Mean height of this sample:', mean(heights[sample1]))\n\n\n[1] \"Joey\"     \"Chandler\" \"Phoebe\"   \"Phoebe\"   \"Joey\"     \"Monica\"  \n\n\n[1] \"Mean height of this sample: 170.333333333333\"\n\n\n\n\nCode\n(sample1 = sample(friends, size = 6, replace = T)); paste('Mean height of this sample:', mean(heights[sample1]))\n\n\n[1] \"Phoebe\" \"Phoebe\" \"Ross\"   \"Phoebe\" \"Rachel\" \"Phoebe\"\n\n\n[1] \"Mean height of this sample: 171.833333333333\""
  },
  {
    "objectID": "11-ml.html#bootstrap-10000-times",
    "href": "11-ml.html#bootstrap-10000-times",
    "title": "bootstrapping",
    "section": "Bootstrap 10,000 Times",
    "text": "Bootstrap 10,000 Times\n\n\nCode\n# When resampling, it is generally a good practice to set random seed\n# for full reproducibility of the resampling process\nset.seed(5067)\n\nboot &lt;- 10000 # Set number of bootstrap samples\n\nfriends = c('Monica', 'Rachel', 'Ross', 'Joey', 'Phoebe', 'Chandler')\nheights &lt;- c(165, 165, 178, 170, 172, 173)\n\nsample_means &lt;- NULL # Initialize list to store sample means"
  },
  {
    "objectID": "11-ml.html#bootstrap-10000-times-1",
    "href": "11-ml.html#bootstrap-10000-times-1",
    "title": "bootstrapping",
    "section": "Bootstrap 10,000 Times",
    "text": "Bootstrap 10,000 Times\n\n\nCode\n# When resampling, it is generally a good practice to set random seed\n# for full reproducibility of the resampling process\nset.seed(5067)\n\nboot &lt;- 10000 # Set number of bootstrap samples\n\nfriends = c('Monica', 'Rachel', 'Ross', 'Joey', 'Phoebe', 'Chandler')\nheights &lt;- c(165, 165, 178, 170, 172, 173)\n\nsample_means &lt;- NULL # Initialize list to store sample means\n\n# Append the mean of bootstrap sample heights to *sample_means*\nfor(i in 1:boot){ #&lt;&lt;\n  this_sample &lt;- sample(heights, size = length(heights), replace = T) #&lt;&lt;\n  sample_means &lt;- c(sample_means, mean(this_sample)) #&lt;&lt;\n} #&lt;&lt;"
  },
  {
    "objectID": "11-ml.html#comparison",
    "href": "11-ml.html#comparison",
    "title": "bootstrapping",
    "section": "Comparison",
    "text": "Comparison\n\n\nCode\nlibrary(ggpubr)\n\n\nLoading required package: ggplot2\n\n\nCode\nmu = mean(heights)\nsem = sd(heights)/sqrt(length(heights))\ncv_t = qt(p = .975, df = length(heights)-1)\n\nbootstrapped = data.frame(means = sample_means) %&gt;%\n  ggplot(aes(x = means)) + \n  geom_histogram(color = \"white\") +\n  geom_density() +\n  geom_vline(aes(xintercept = mean(sample_means), color = \"mean\"), \n             linewidth = 2) +\n  geom_vline(aes(xintercept = median(sample_means), color = \"median\"),\n             linewidth = 2) +\n  geom_vline(aes(xintercept = quantile(sample_means, probs = .025), color = \"Lower 2.5%\"), \n             linewidth = 2) +\n    geom_vline(aes(xintercept = quantile(sample_means, probs = .975), color = \"Upper 2.5%\"), \n               linewidth = 2) +\n  scale_x_continuous(limits = c(mu-3*sem, mu+3*sem))+\n  ggtitle(\"Bootstrapped distribution\") +\n  cowplot::theme_cowplot()\n\nfrom_prob = data.frame(means = seq(from = min(sample_means), to = max(sample_means))) %&gt;%\n  ggplot(aes(x = means)) +\n  stat_function(fun = function(x) dnorm(x, m = mu, sd = sem)) + \n  geom_vline(aes(xintercept = mean(heights), color = \"mean\"), \n             linewidth = 2) +\n  geom_vline(aes(xintercept = median(heights), color = \"median\"), \n             linewidth = 2) +\n  geom_vline(aes(xintercept = mu-cv_t*sem, color = \"Lower 2.5%\"), \n             linewidth = 2) +\n  geom_vline(aes(xintercept = mu+cv_t*sem, color = \"Upper 2.5%\"), \n             linewidth = 2) + \n  scale_x_continuous(limits = c(mu-3*sem, mu+3*sem))+  \n  ggtitle(\"Distribution from probability theory\") +\n  cowplot::theme_cowplot()\n\nggarrange(bootstrapped, from_prob, ncol = 1)\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 3 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 3 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\n\n\n\n\nThere are several candidates for central tendency (e.g., mean, median) and for variability (e.g., standard deviation, interquartile range). Some of these do not have well understood theoretical sampling distributions.\nFor the mean and standard deviation, we have theoretical sampling distributions to help us, provided we think the mean and standard deviation are the best indices. For the others, we can use bootstrapping."
  },
  {
    "objectID": "11-ml.html#example-2",
    "href": "11-ml.html#example-2",
    "title": "bootstrapping",
    "section": "Example 2",
    "text": "Example 2\nCentral tendency and variability of 216 reaction times\n\n\nCode\n# Set random seed before generating data\nset.seed(5067)\n\n# The observations generally follow the F Distribution + random noise\nresponse = rf(n = 216, 3, 50) \nresponse = response * 500 + rnorm(n = 216, mean = 200, sd = 100)"
  },
  {
    "objectID": "11-ml.html#visualize-data",
    "href": "11-ml.html#visualize-data",
    "title": "bootstrapping",
    "section": "Visualize Data",
    "text": "Visualize Data\n\n\nCode\nlibrary(tidyverse)\n\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.5\n‚úî forcats   1.0.0     ‚úî stringr   1.5.1\n‚úî lubridate 1.9.4     ‚úî tibble    3.2.1\n‚úî purrr     1.0.2     ‚úî tidyr     1.3.1\n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nset.seed(1048596)\nresponse = rf(n = 216, 3, 50) \nresponse = response * 500 + rnorm(n = 216, mean = 200, sd = 100)\n\nvalues = quantile(response, \n                  probs = c(.025, .5, .975))\nmean_res = mean(response)\n\ndata.frame(x = response) %&gt;%\n  ggplot(aes(x)) +\n  geom_histogram(aes(y = ..density..), \n                 binwidth = 150, \n                 fill = \"lightgrey\",\n                 color = \"black\")+\n  geom_density()+\n  geom_vline(aes(xintercept = values[1], \n                 color = \"Lower 2.5%\"), linewidth = 2)+\n  geom_vline(aes(xintercept = values[2], color = \"Median\"), \n             linewidth = 2)+\n  geom_vline(aes(xintercept = values[3], color = \"Upper 2.5%\"),\n             linewidth = 2)+\n  geom_vline(aes(xintercept = mean_res, color = \"Mean\"), \n             linewidth = 2)+\n  labs(x = \"Reponse time (ms)\", title = \"Response Time Distribution\") + cowplot::theme_cowplot(font_size = 20)\n\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `after_stat(density)` instead.\n\n\n\n\n\n\n\n\nCode\nset.seed(5067) # Set random seed\nboot &lt;- 10000 # Set number of bootstrap samples\n\nresponse_means &lt;- NULL # Initialize list of mean values\n\nfor(i in 1:boot){\n  sample_response &lt;- sample(response, size = 216, replace = T)\n  response_means &lt;- c(response_means, mean(sample_response))\n}\n\n\nWhat is the bootstrap mean and its 95% CI?\n\n\nCode\nmean(response_means)\n\n\n[1] 682.5499\n\n\nCode\nquantile(response_means, probs = c(.025, .975))\n\n\n    2.5%    97.5% \n633.2482 732.0461"
  },
  {
    "objectID": "11-ml.html#distribution-of-means",
    "href": "11-ml.html#distribution-of-means",
    "title": "bootstrapping",
    "section": "Distribution of Means",
    "text": "Distribution of Means\n\n\nCode\ndata.frame(means = response_means) %&gt;%\n  ggplot(aes(x = means)) + \n  geom_histogram(color = \"white\") +\n  geom_density() +\n  geom_vline(aes(xintercept = mean(response_means), color = \"mean\"),\n             linewidth = 2) +\n  geom_vline(aes(xintercept = median(response_means), color = \"median\"),\n             linewidth = 2) +\n  geom_vline(aes(xintercept = quantile(response_means, probs = .025), \n                 color = \"Lower 2.5%\"), linewidth = 2) +\n  geom_vline(aes(xintercept = quantile(response_means, probs = .975), \n                 color = \"Upper 2.5%\"), linewidth = 2) +\n  cowplot::theme_cowplot()\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\nset.seed(5067)\nboot &lt;- 10000\nresponse_med &lt;- NULL\n\nfor(i in 1:boot){\n  sample_response &lt;- sample(response, size = 216, replace = T)\n  response_med &lt;- c(response_med, median(sample_response))}\n\n\n\n\nCode\ndata.frame(medians = response_med) %&gt;%\n  ggplot(aes(x = medians)) + \n  geom_histogram(aes(y = ..density..),\n                 color = \"white\", fill = \"grey\") +\n  geom_density() +\n  geom_vline(aes(xintercept = mean(response_med), color = \"mean\"), \n             linewidth = 2) +\n  geom_vline(aes(xintercept = median(response_med), color = \"median\"),\n             linewidth = 2) +\n  geom_vline(aes(xintercept = quantile(response_med, probs = .025), color = \"Lower 2.5%\"), linewidth = 2) +\n    geom_vline(aes(xintercept = quantile(response_med, probs = .975), color = \"Upper 2.5%\"), linewidth = 2) +\n  cowplot::theme_cowplot()\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\nmean(response_med)\n\n[1] 600.4248\n\nmedian(response_med)\n\n[1] 598.9464\n\nquantile(response_med, \n         probs = c(.025, .975))\n\n    2.5%    97.5% \n542.0469 665.4593 \n\n\n\n\nset.seed(5067)\nboot &lt;- 10000\nresponse_sd &lt;- NULL\n\nfor(i in 1:boot){\n  sample_response &lt;- sample(response, size = 216, replace = T)\n  response_sd &lt;- c(response_sd, sd(sample_response))}\n\n\n\n\nCode\ndata.frame(sds = response_sd) %&gt;%\n  ggplot(aes(x = sds)) + \n  geom_histogram(aes(y = ..density..),color = \"white\", fill = \"grey\") +\n  geom_density() +\n  geom_vline(aes(xintercept = mean(response_sd), color = \"mean\"), \n             linewidth = 2) +\n  geom_vline(aes(xintercept = median(response_sd), \n                 color = \"median\"), linewidth = 2) +\n  geom_vline(aes(xintercept = quantile(response_sd, probs = .025), \n                 color = \"Lower 2.5%\"), linewidth = 2) +\n  geom_vline(aes(xintercept = quantile(response_sd, probs = .975), \n                 color = \"Upper 2.5%\"), linewidth = 2) +\n  cowplot::theme_cowplot()\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nCode\nmean(response_sd)\n\n\n[1] 374.7614\n\n\nCode\nmedian(response_sd)\n\n\n[1] 374.299\n\n\nCode\nquantile(response_sd, \n         probs = c(.025, .975))\n\n\n    2.5%    97.5% \n326.6762 425.9399"
  },
  {
    "objectID": "11-ml.html#other-estimators",
    "href": "11-ml.html#other-estimators",
    "title": "bootstrapping",
    "section": "Other Estimators?",
    "text": "Other Estimators?\nYou can bootstrap estimates and 95% confidence intervals for any statistics you‚Äôll need to estimate.\nThings you should learn how to do in R:\n\nlearn to read a for loop.\nlearn to write your own function.\n\n\nThe boot package and function provides some help to speed this process along. Things you should learn how to do in R:\n\nlibrary(boot)\n\n# function to obtain R-Squared from the data\nrsq &lt;- function(data, indices) {\n  d &lt;- data[indices,] # allows boot to select sample\n  fit &lt;- lm(mpg ~ wt + disp, data = d) # this is the code you would have run\n  return(summary(fit)$r.square)\n}\n\nresults &lt;- boot(data = mtcars, statistic = rsq, R = 10000)\n\n\n\n\nCode\ndata.frame(rsq = results$t) %&gt;%\n  ggplot(aes(x = rsq)) +\n  geom_histogram(color = \"white\", bins = 30) \n\n\n\n\n\n\n\n\nCode\nmedian(results$t)\n\n\n[1] 0.7972849\n\n\nCode\nboot.ci(results, type = \"perc\")\n\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 10000 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = results, type = \"perc\")\n\nIntervals : \nLevel     Percentile     \n95%   ( 0.6862,  0.8771 )  \nCalculations and Intervals on Original Scale"
  },
  {
    "objectID": "11-ml.html#another-example",
    "href": "11-ml.html#another-example",
    "title": "bootstrapping",
    "section": "Another Example",
    "text": "Another Example\nIn this district, Verizon provides line service to both Verizon and non-Verizon customers. Here, we are going to look at a dataset containing service waiting times for Verizon customers (ILEC) and non-Verizon customers (CLEC). We are interested in whether waiting time of non-Verizon customers is longer than that of Verizon customers.\n\n\nCode\nVerizon = read.csv(\"https://raw.githubusercontent.com/shellyc26/psy5067/master/data/Verizon.csv\")\n\n\n\nhead(Verizon, 3)\n\n  Time Group\n1 17.5  ILEC\n2  2.4  ILEC\n3  0.0  ILEC"
  },
  {
    "objectID": "11-ml.html#inspect-verizon-data-1",
    "href": "11-ml.html#inspect-verizon-data-1",
    "title": "bootstrapping",
    "section": "Inspect Verizon Data 1",
    "text": "Inspect Verizon Data 1\n\n\nCode\nVerizon %&gt;%\n  ggplot(aes(x = Time, fill = Group)) + \n  geom_histogram(bins = 30) + \n  guides(fill = \"none\") +\n  facet_wrap(~Group, scales = \"free_y\")\n\n\n\n\n\n\nLeft is the distribution of waiting times of Non-Verizon (CLEC) customers and right is the distribution of waiting times of Verizon (ILEC) customers"
  },
  {
    "objectID": "11-ml.html#inspect-verizon-data-2",
    "href": "11-ml.html#inspect-verizon-data-2",
    "title": "bootstrapping",
    "section": "Inspect Verizon Data 2",
    "text": "Inspect Verizon Data 2\n\n\nCode\nVerizon %&gt;%\n  ggplot(aes(x = Time, fill = Group)) + \n  geom_histogram(bins = 50, position = \"dodge\") + \n  guides(fill = \"none\") +\n  theme_bw()\n\n\n\n\n\n\n\nCode\ntable(Verizon$Group)\n\n\n\nCLEC ILEC \n  23 1664"
  },
  {
    "objectID": "11-ml.html#analysis-plan-and-justification",
    "href": "11-ml.html#analysis-plan-and-justification",
    "title": "bootstrapping",
    "section": "Analysis Plan and Justification",
    "text": "Analysis Plan and Justification\nIt seems that the data do not meet the typical assumptions of an independent samples \\(t\\)-test.\nIn this case, to estimate mean differences we can use bootstrapping.\nHere, we‚Äôll resample with replacement separately from the two samples and calculate their difference in means.\n\n\n\nCode\n# Set random seed and number of bootstrap samples\nset.seed(5067)\nboot &lt;- 10000\n\nresponse_means &lt;- NULL\n\nfor(i in 1:boot){\n  sample_response &lt;- sample(response, size = 216, replace = T)\n  response_means &lt;- c(response_means,mean(sample_response))\n}\n\n\nInside the For Loop\n\nSample (with replacement) Verizon group (ILEC) customers\nSample (with replacement) non-Verizon group (CLEC) customers\nCalculate the difference in means between the two groups\nAppend the difference value to a list"
  },
  {
    "objectID": "11-ml.html#one-solution",
    "href": "11-ml.html#one-solution",
    "title": "bootstrapping",
    "section": "One Solution",
    "text": "One Solution\n\nset.seed(5067)\nboot &lt;- 10000\ndifference &lt;- NULL\n\nsubsample_CLEC = Verizon %&gt;% filter(Group == \"CLEC\") #&lt;&lt;\nsubsample_ILEC = Verizon %&gt;% filter(Group == \"ILEC\") #&lt;&lt;"
  },
  {
    "objectID": "11-ml.html#one-solution-1",
    "href": "11-ml.html#one-solution-1",
    "title": "bootstrapping",
    "section": "One Solution",
    "text": "One Solution\n\nset.seed(5067)\nboot &lt;- 10000\ndifference &lt;- NULL\n\nsubsample_CLEC = Verizon %&gt;% filter(Group == \"CLEC\")\nsubsample_ILEC = Verizon %&gt;% filter(Group == \"ILEC\")\n\nfor(i in 1:boot){\n  # Sample (with replacement) Verizon group (ILEC) customers\n  sample_CLEC = sample(subsample_CLEC$Time, #&lt;&lt;\n                       size = nrow(subsample_CLEC), #&lt;&lt;\n                       replace = T) #&lt;&lt;\n  # Sample (with replacement) Non-Verizon group (CLEC) customers\n  sample_ILEC = sample(subsample_ILEC$Time, #&lt;&lt;\n                       size = nrow(subsample_ILEC), #&lt;&lt;\n                       replace = T) #&lt;&lt;\n}"
  },
  {
    "objectID": "11-ml.html#one-solution-2",
    "href": "11-ml.html#one-solution-2",
    "title": "bootstrapping",
    "section": "One Solution",
    "text": "One Solution\n\nset.seed(5067)\nboot &lt;- 10000\ndifference &lt;- NULL\n\nsubsample_CLEC = Verizon %&gt;% filter(Group == \"CLEC\")\nsubsample_ILEC = Verizon %&gt;% filter(Group == \"ILEC\")\n\nfor(i in 1:boot){\n  # Sample (with replacement) Verizon group (ILEC) customers\n  sample_CLEC = sample(subsample_CLEC$Time, \n                       size = nrow(subsample_CLEC), \n                       replace = T)\n  # Sample (with replacement) Non-Verizon group (CLEC) customers\n  sample_ILEC = sample(subsample_ILEC$Time, \n                       size = nrow(subsample_ILEC), \n                       replace = T)\n  \n  # Calculate the difference in means between the two groups\n  # Append the difference value to a list\n  difference &lt;- c(difference, mean(sample_CLEC) - mean(sample_ILEC)) #&lt;&lt;\n}"
  },
  {
    "objectID": "11-ml.html#bootstrap-distribution-of-differences",
    "href": "11-ml.html#bootstrap-distribution-of-differences",
    "title": "bootstrapping",
    "section": "Bootstrap Distribution of Differences",
    "text": "Bootstrap Distribution of Differences\n\n\nCode\ndata.frame(differences = difference) %&gt;%\n  ggplot(aes(x = differences)) + \n  geom_histogram(aes(y = ..density..),color = \"white\", fill = \"grey\") +\n  geom_density() +\n  geom_vline(aes(xintercept = mean(differences), color = \"mean\"), \n             linewidth = 2) +\n  geom_vline(aes(xintercept = median(differences), color = \"median\"),\n             linewidth = 2) +\n  geom_vline(aes(xintercept = quantile(differences, probs = .025), color = \"Lower 2.5%\"), \n             linewidth = 2) +\n    geom_vline(aes(xintercept = quantile(differences, probs = .975), color = \"Upper 2.5%\"), \n               linewidth = 2) +\n  cowplot::theme_cowplot()\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nThe difference in means is 7.64 \\([1.66,16.98]\\). What would this mean?\n\nBootstrapping is useful when:\n\nViolated assumptions of the test (e.g., normality)\nYou have good reason to believe the sampling distribution is not normal, but don‚Äôt know what it is (e.g., median)\nOddities in your data, like very unbalanced samples\n\nThis allows you to create a confidence interval around any statistic you want. You can test whether these statistics are significantly different from any other value.\n\nBootstrapping will NOT help you deal with:\n\nDependence between observations ‚Äì for this, you‚Äôll need to explicitly model dependence\nImproperly specified models or forms ‚Äì use theory to guide you here\nMeasurement error ‚Äì why bother?\nCaveats: representativeness of the sample, outliers"
  },
  {
    "objectID": "11-ml.html#jackknife-resampling",
    "href": "11-ml.html#jackknife-resampling",
    "title": "bootstrapping",
    "section": "Jackknife Resampling",
    "text": "Jackknife Resampling\nJackknife Resampling is a method where researchers generate n sub-samples, each leaving out one observation. The method is very similar to bootstrapping except the way that we create the sub-samples.\n\nFriends Example. If you decide to jackknife their heights, you would draw six sub-samples and calculate their respective mean height.\n\n\nCode\nfriends = c('Monica', 'Rachel', 'Ross', 'Joey', 'Phoebe', 'Chandler')\nheights = c(165, 165, 178, 170, 172, 173)\nnames(heights) = friends\n\n\n\n\nCode\npaste(\"First Sub-sample: \", toString(friends[-1]))\n\n\n[1] \"First Sub-sample:  Rachel, Ross, Joey, Phoebe, Chandler\"\n\n\n\n\nCode\npaste(\"Second Sub-sample: \", toString(friends[-2]))\n\n\n[1] \"Second Sub-sample:  Monica, Ross, Joey, Phoebe, Chandler\"\n\n\n\n\n[1] \"Third Sub-sample:  Monica, Rachel, Joey, Phoebe, Chandler\"\n\n\n\n\n[1] \"Fourth Sub-sample:  Monica, Rachel, Ross, Phoebe, Chandler\"\n\n\n\n\n[1] \"Fifth Sub-sample:  Monica, Rachel, Ross, Joey, Chandler\"\n\n\n\n\n[1] \"Sixth Sub-sample:  Monica, Rachel, Ross, Joey, Phoebe\"\n\n\nNotice that there are 6 sub-samples of size 5."
  },
  {
    "objectID": "11-ml.html#jack-of-all-trades-master-of-none---jackknife",
    "href": "11-ml.html#jack-of-all-trades-master-of-none---jackknife",
    "title": "bootstrapping",
    "section": "Jack of all trades, master of none - Jackknife",
    "text": "Jack of all trades, master of none - Jackknife\n\nCan assess the accuracy of a statistical estimator without making assumptions about the underlying distribution\nComputationally efficient bc only n sub-samples are generated (compared to 10,000 for example)\nNot used as much these days\nSensitive to sample size: If sample size is small, it can result in inaccurate estimates of the bias (e.g., sample size of 6 means six jackknife samples)."
  },
  {
    "objectID": "11-ml.html#permutation-testing",
    "href": "11-ml.html#permutation-testing",
    "title": "bootstrapping",
    "section": "Permutation Testing",
    "text": "Permutation Testing\nA resampling method that involves randomly shuffling the labels (e.g., conditions) across the data and recomputing the test statistic of interest, thereby deriving a null distribution of the test statistic."
  },
  {
    "objectID": "11-ml.html#permutation-example-restaurants",
    "href": "11-ml.html#permutation-example-restaurants",
    "title": "bootstrapping",
    "section": "Permutation Example: Restaurants",
    "text": "Permutation Example: Restaurants\nWe are interested in the difference in the rating of group A and group B restaurants.\n\n\nCode\nnames &lt;- c(\"Pappy's Smokehouse\", 'Mai Lee', \"Adriana's on the Hill\", 'Salt & Smoke', 'Chilli Spot', 'BLK MKT')\nlabels &lt;- c('A', 'A', 'A', 'B', 'B', 'B')\nratings &lt;- c(7, 8, 8, 4, 6, 7)\n\nperm_df &lt;- data.frame(name = names, \n                      group = labels, \n                      rating = ratings)\n\nhead(perm_df, 6)\n\n\n                   name group rating\n1    Pappy's Smokehouse     A      7\n2               Mai Lee     A      8\n3 Adriana's on the Hill     A      8\n4          Salt & Smoke     B      4\n5           Chilli Spot     B      6\n6               BLK MKT     B      7"
  },
  {
    "objectID": "11-ml.html#permutation-example-restaurants-1",
    "href": "11-ml.html#permutation-example-restaurants-1",
    "title": "bootstrapping",
    "section": "Permutation Example: Restaurants",
    "text": "Permutation Example: Restaurants\nThe observed difference is‚Ä¶\n\n\n                   name group rating\n1    Pappy's Smokehouse     A      7\n2               Mai Lee     A      8\n3 Adriana's on the Hill     A      8\n\n\n\n\n[1] 7.666667\n\n\n\n\n          name group rating\n1 Salt & Smoke     B      4\n2  Chilli Spot     B      6\n3      BLK MKT     B      7\n\n\n\n\n[1] 5.666667\n\n\n\nperm_df %&gt;% filter(group == 'A') %&gt;% pull(rating) %&gt;% mean() - perm_df %&gt;% filter(group == 'B') %&gt;% pull(rating) %&gt;% mean()\n\n[1] 2\n\n\n\nInitially, the three restaurants in group A were:\n\n\n                   name group rating\n1    Pappy's Smokehouse     A      7\n2               Mai Lee     A      8\n3 Adriana's on the Hill     A      8\n\n\n\nNow, we are going to randomly assign restaurants to group A and group B, and calculate the mean difference in ratings.\n\n\nCode\nrandom_indices &lt;- sample.int(nrow(perm_df), 3)\nrandom_A &lt;- perm_df[random_indices,]\nrandom_B &lt;- perm_df[-random_indices,]\n\nrandom_A %&gt;% pull(name)\n\n\n[1] \"Salt & Smoke\" \"Mai Lee\"      \"Chilli Spot\" \n\n\n\n\nCode\nrandom_A %&gt;% pull(rating) %&gt;% mean() - random_B %&gt;% pull(rating) %&gt;% mean()\n\n\n[1] -1.333333\n\n\n\nRepeat the previous step numerous times to impose a null distribution of mean differences\n\nobserved_diff &lt;- perm_df %&gt;% filter(group == 'A') %&gt;% pull(rating) %&gt;% mean() - perm_df %&gt;% filter(group == 'B') %&gt;% pull(rating) %&gt;% mean()\n\nset.seed(5067) # Set random seed\nperm = 20 # Set number of permutations\n\ndifferences &lt;- NULL\n\nfor (i in c(1:perm)){\n  random_indices &lt;- sample.int(nrow(perm_df), 3)\n  random_A &lt;- perm_df[random_indices,]\n  random_B &lt;- perm_df[-random_indices,]\n  \n  differences &lt;- c(random_A %&gt;% pull(rating) %&gt;% mean() - random_B %&gt;% pull(rating) %&gt;% mean(), differences)\n}\n\n\n\n\nCode\ndiff_df = data.frame(diff = differences)\n\nggplot(diff_df, aes(x = diff)) +\n  geom_histogram(color = \"white\") + \n  geom_vline(aes(xintercept = observed_diff, color = \"observed\"), \n             linewidth = 1) +\n  ggtitle(\"Empirical Null Distribution of Differences\") +\n  cowplot::theme_cowplot()\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nCount the number of permutations that have larger test statistic value than the observed difference, equivalent to the \\(p\\)-value (or the probability that a test statistic is greater than or equal to the observed value, under the null)."
  },
  {
    "objectID": "11-ml.html#lets-try",
    "href": "11-ml.html#lets-try",
    "title": "bootstrapping",
    "section": "Let‚Äôs Try",
    "text": "Let‚Äôs Try\nInitially, we had 23 CLEC and 1,664 ILEC customers. First, we are going to calculate the mean difference in waiting time between the two groups using the observed data.\n\nsubsample_CLEC = Verizon %&gt;% filter(Group == \"CLEC\")\nsubsample_ILEC = Verizon %&gt;% filter(Group == \"ILEC\")\n\n(mean(subsample_CLEC$Time) - mean(subsample_ILEC$Time))\n\n[1] 8.09752"
  },
  {
    "objectID": "11-ml.html#second-step",
    "href": "11-ml.html#second-step",
    "title": "bootstrapping",
    "section": "Second Step",
    "text": "Second Step\nWe are going to randomly shuffle the labels of groups (previous CLEC and ILEC labels don‚Äôt matter!). Label 23 random customers as CLEC and 1,664 random customers as ILEC 1,000 times, and store the mean difference in waiting time between the two groups inside a list.\n\nOne iteration would look something like this:\n\nrandom_indices &lt;- sample.int(nrow(Verizon), 23)\nrandom_CLEC &lt;- Verizon[random_indices,]\nrandom_ILEC &lt;- Verizon[-random_indices,]\n\n(mean(random_CLEC$Time) - mean(random_ILEC$Time))\n\n[1] 2.394109"
  },
  {
    "objectID": "11-ml.html#repeat-1000-times-to-derive-the-p-value",
    "href": "11-ml.html#repeat-1000-times-to-derive-the-p-value",
    "title": "bootstrapping",
    "section": "Repeat 1,000 times to derive the p-value",
    "text": "Repeat 1,000 times to derive the p-value\nOne iteration would look something like this:\n\nrandom_indices &lt;- sample.int(nrow(Verizon), 23)\nrandom_CLEC &lt;- Verizon[random_indices,]\nrandom_ILEC &lt;- Verizon[-random_indices,]\n\n(mean(random_CLEC$Time) - mean(random_ILEC$Time))\n\n[1] -0.3775922"
  },
  {
    "objectID": "11-ml.html#analysis-plan",
    "href": "11-ml.html#analysis-plan",
    "title": "bootstrapping",
    "section": "Analysis Plan",
    "text": "Analysis Plan\n\nPut this in a for loop\n\n‚Äì\n\nConstruct a list of mean differences\n\n‚Äì\n\nDetermine the number of (random) mean differences greater than the observed difference in means (8.10)."
  },
  {
    "objectID": "11-ml.html#one-way-to-do-this",
    "href": "11-ml.html#one-way-to-do-this",
    "title": "bootstrapping",
    "section": "One Way to do this",
    "text": "One Way to do this\n\nsubsample_CLEC = Verizon %&gt;% filter(Group == \"CLEC\")\nsubsample_ILEC = Verizon %&gt;% filter(Group == \"ILEC\")\nobserved_diff &lt;- mean(subsample_CLEC$Time) - mean(subsample_ILEC$Time)\n\nset.seed(1048596) # Set random seed\nperm = 1000 # Set number of permutations\n\ndifferences &lt;- NULL #&lt;&lt;"
  },
  {
    "objectID": "11-ml.html#one-way-to-do-this-1",
    "href": "11-ml.html#one-way-to-do-this-1",
    "title": "bootstrapping",
    "section": "One Way to do this",
    "text": "One Way to do this\n\nsubsample_CLEC = Verizon %&gt;% filter(Group == \"CLEC\")\nsubsample_ILEC = Verizon %&gt;% filter(Group == \"ILEC\")\nobserved_diff &lt;- mean(subsample_CLEC$Time) - mean(subsample_ILEC$Time)\n\nset.seed(1048596) # Set random seed\nperm = 1000 # Set number of permutations\n\ndifferences &lt;- NULL\n\nfor (i in 1:perm){\n  random_indices &lt;- sample.int(nrow(Verizon), 23)\n  random_CLEC &lt;- Verizon[random_indices,]\n  random_ILEC &lt;- Verizon[-random_indices,]\n  \n  differences &lt;- c(differences, #&lt;&lt;\n                   mean(random_CLEC$Time) - mean(random_ILEC$Time)) #&lt;&lt;\n}\n\n\n\nsubsample_CLEC = Verizon %&gt;% filter(Group == \"CLEC\")\nsubsample_ILEC = Verizon %&gt;% filter(Group == \"ILEC\")\nobserved_diff &lt;- mean(subsample_CLEC$Time) - mean(subsample_ILEC$Time)\n\nset.seed(1048596) # Set random seed\nperm = 1000 # Set number of permutations\n\ndifferences &lt;- NULL\n\nfor (i in 1:perm){\n  random_indices &lt;- sample.int(nrow(Verizon), 23)\n  random_CLEC &lt;- Verizon[random_indices,]\n  random_ILEC &lt;- Verizon[-random_indices,]\n  \n  differences &lt;- c(differences, \n                   mean(random_CLEC$Time) - mean(random_ILEC$Time))\n}\n\npaste(\"Number of mean differences greater than observed difference: \", sum(differences &gt; observed_diff)) #&lt;&lt;\n\n[1] \"Number of mean differences greater than observed difference:  24\"\n\npaste(\"p-value: \", sum(differences &gt; observed_diff)/length(differences)) #&lt;&lt;\n\n[1] \"p-value:  0.024\""
  },
  {
    "objectID": "11-ml.html#visualization",
    "href": "11-ml.html#visualization",
    "title": "bootstrapping",
    "section": "Visualization",
    "text": "Visualization\n\n\nCode\nperm_df = data.frame(diff = differences)\n\nggplot(perm_df, aes(x = diff)) +\n  geom_histogram(color = \"white\") + \n  geom_vline(aes(xintercept = mean(diff), color = \"mean\"), \n             linewidth = 1) +\n  geom_vline(aes(xintercept = median(diff), color = \"median\"), \n             linewidth = 1) +\n  geom_vline(aes(xintercept = observed_diff, color = \"observed\"), \n             linewidth = 1) +\n  ggtitle(\"Empirical Distribution of Differences under the Null\") +\n  cowplot::theme_cowplot()\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "11-ml.html#permutation-testing-summary",
    "href": "11-ml.html#permutation-testing-summary",
    "title": "bootstrapping",
    "section": "Permutation Testing Summary",
    "text": "Permutation Testing Summary\n\nPermutation testing is useful for hypothesis testing because we can easily derive a \\(p\\)-value\nCan be used when data violates common assumptions about the data (i.e., homogeneity of variance and normality)\nAssumption that the observations need to be exchangeable. Some observations may not be exchangeable (e.g., time series data - data collected at different time points)\nSample size needs to be large. No point randomly shuffling 1,000 times when the possible permutation is less than 1000."
  },
  {
    "objectID": "2-Basics.html",
    "href": "2-Basics.html",
    "title": "GLM basics",
    "section": "",
    "text": "Our DV (here forth Y) is what we are trying to understand\nWe hypothesize it has some relationship with your IV(s) (here forth Xs), with what is left over described as error (E)\n\n\\(y = b_0 + b_{1}X + e\\)"
  },
  {
    "objectID": "2-Basics.html#thinking-in-terms-of-models",
    "href": "2-Basics.html#thinking-in-terms-of-models",
    "title": "GLM basics",
    "section": "",
    "text": "Our DV (here forth Y) is what we are trying to understand\nWe hypothesize it has some relationship with your IV(s) (here forth Xs), with what is left over described as error (E)\n\n\\(y = b_0 + b_{1}X + e\\)"
  },
  {
    "objectID": "2-Basics.html#how-can-we-visualize-data-to-make-sense-of-it",
    "href": "2-Basics.html#how-can-we-visualize-data-to-make-sense-of-it",
    "title": "GLM basics",
    "section": "How can we visualize data to make sense of it?",
    "text": "How can we visualize data to make sense of it?\n\n\nCode\nlibrary(broom)\nset.seed(123)\nx.1 &lt;- rnorm(100, 0, 1)\ne.1 &lt;- rnorm(100, 0, 2)\ny.1 &lt;- .5 + .55 * x.1 + e.1\nd.1 &lt;- data.frame(x.1,y.1)\nm.1 &lt;- lm(y.1 ~ x.1, data = d.1)\nd1.f&lt;- augment(m.1)\nd.1\n\n\n             x.1         y.1\n1   -0.560475647 -1.22907473\n2   -0.230177489  0.88716980\n3    1.558708314  0.86390582\n4    0.070508391 -0.15630558\n5    0.129287735 -1.33212888\n6    1.715064987  1.35323029\n7    0.460916206 -0.81630503\n8   -1.265061235 -3.53166755\n9   -0.686852852 -0.63822211\n10  -0.445661970  2.09287913\n11   1.224081797  0.02255106\n12   0.359813827  1.91382625\n13   0.400771451 -2.51534112\n14   0.110682716  0.44975156\n15  -0.555841135  1.23310178\n16   1.786913137  2.08510895\n17   0.497850478  0.98517015\n18  -1.966617157 -1.86305145\n19   0.701355902 -0.81366295\n20  -0.472791408 -1.80829286\n21  -1.067823706  0.14799016\n22  -0.217974915 -1.51483543\n23  -1.026004448 -1.04541733\n24  -0.728891229 -0.41307456\n25  -0.625039268  3.84395241\n26  -1.686693311 -1.73158112\n27   0.837787044  1.43155602\n28   0.153373118  0.74027691\n29  -1.138136937 -2.04968858\n30   1.253814921  1.04698203\n31   0.426464221  3.62365704\n32  -0.295071483  1.24071879\n33   0.895125661  1.07478496\n34   0.878133488  0.13797975\n35   0.821581082 -3.15462485\n36   0.688640254  3.14142657\n37   0.553917654 -2.11662543\n38  -0.061911711  1.94584358\n39  -0.305962664  4.14992767\n40  -0.380471001 -2.59704537\n41  -0.694706979  1.52147983\n42  -0.207917278 -0.13874948\n43  -1.265396352 -3.34025631\n44   2.168955965 -1.33640953\n45   1.207961998 -2.03869325\n46  -1.123108583 -1.17952277\n47  -0.402884835 -2.64509783\n48  -0.466655354  1.61917310\n49   0.779965118  5.12919870\n50  -0.083369066 -2.11991394\n51   0.253318514  2.21480288\n52  -0.028546755  2.02238377\n53  -0.042870457  1.14082641\n54   1.368602284 -0.76402196\n55  -0.225770986  0.13692074\n56   1.516470604  0.77326816\n57  -1.548752804  0.77416502\n58   0.584613750  0.07666005\n59   0.123854244  2.52206661\n60   0.215941569 -0.13039385\n61   0.379639483  2.81422465\n62  -0.502323453 -1.87463191\n63  -0.333207384 -2.20357455\n64  -1.018575383  6.42186341\n65  -1.071791226 -0.92320035\n66   0.303528641  1.26339594\n67   0.448209779  2.01965473\n68   0.053004227 -0.43840893\n69   0.922267468  2.04097120\n70   2.050084686  2.36547563\n71  -0.491031166 -0.20082816\n72  -2.309168876 -0.63945681\n73   1.005738524  0.98502168\n74  -0.709200763  4.36684338\n75  -0.688008616 -1.36107693\n76   1.025571370 -1.12792828\n77  -0.284773007  0.41895164\n78  -1.220717712  0.44956676\n79   0.181303480  1.47276387\n80  -0.138891362 -0.49312091\n81   0.005764186 -1.62348197\n82   0.385280401  3.23827457\n83  -0.370660032 -0.40316379\n84   0.644376549 -0.87661862\n85  -0.220486562 -0.09382675\n86   0.331781964  0.28812829\n87   1.096839013  3.32310204\n88   0.435181491  0.90882440\n89  -0.325931586  1.82884520\n90   1.148807618  0.13326016\n91   0.993503856  1.47531774\n92   0.548396960  0.15224650\n93   0.238731735  0.82046951\n94  -0.627906076 -1.63607506\n95   1.360652449 -1.37324422\n96  -0.600259587  4.16428400\n97   2.187332993  2.90445079\n98   1.532610626 -1.15960688\n99  -0.235700359 -0.85196703\n100 -1.026420900 -2.43549166\n\n\n\n\n\nCode\nlibrary(tidyverse)\n\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.5\n‚úî forcats   1.0.0     ‚úî stringr   1.5.1\n‚úî ggplot2   3.5.1     ‚úî tibble    3.2.1\n‚úî lubridate 1.9.4     ‚úî tidyr     1.3.1\n‚úî purrr     1.0.2     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nggplot(d1.f , aes(x=x.1, y=y.1)) +\n    geom_point(size = 2)\n\n\n\n\n\n\n\n\nCode\nggplot(d1.f , aes(x=x.1, y=y.1)) +\n    geom_point(size = 2) +\n  geom_smooth(method = lm, se = FALSE) \n\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "2-Basics.html#how-do-we-visualize-categorical-data",
    "href": "2-Basics.html#how-do-we-visualize-categorical-data",
    "title": "GLM basics",
    "section": "How do we visualize categorical data?",
    "text": "How do we visualize categorical data?\nNominal/categorical data does not have any inherent numbers associated with it. Think control/tx, eye color, etc.\n\n\nCode\nset.seed(123)\ngroup &lt;- c(0, 1)\nx.2 &lt;- rep(group, times = 50)\ne.1 &lt;- rnorm(100, 0, 1)\ny.1 &lt;- .5 + .85 * x.2 + e.1\nd.2 &lt;- data.frame(x.2,y.1)\nm.2 &lt;- lm(y.1 ~ x.2, data = d.2)\nd2.f&lt;- augment(m.2)\nd.2\n\n\n    x.2          y.1\n1     0 -0.060475647\n2     1  1.119822511\n3     0  2.058708314\n4     1  1.420508391\n5     0  0.629287735\n6     1  3.065064987\n7     0  0.960916206\n8     1  0.084938765\n9     0 -0.186852852\n10    1  0.904338030\n11    0  1.724081797\n12    1  1.709813827\n13    0  0.900771451\n14    1  1.460682716\n15    0 -0.055841135\n16    1  3.136913137\n17    0  0.997850478\n18    1 -0.616617157\n19    0  1.201355902\n20    1  0.877208592\n21    0 -0.567823706\n22    1  1.132025085\n23    0 -0.526004448\n24    1  0.621108771\n25    0 -0.125039268\n26    1 -0.336693311\n27    0  1.337787044\n28    1  1.503373118\n29    0 -0.638136937\n30    1  2.603814921\n31    0  0.926464221\n32    1  1.054928517\n33    0  1.395125661\n34    1  2.228133488\n35    0  1.321581082\n36    1  2.038640254\n37    0  1.053917654\n38    1  1.288088289\n39    0  0.194037336\n40    1  0.969528999\n41    0 -0.194706979\n42    1  1.142082722\n43    0 -0.765396352\n44    1  3.518955965\n45    0  1.707961998\n46    1  0.226891417\n47    0  0.097115165\n48    1  0.883344646\n49    0  1.279965118\n50    1  1.266630934\n51    0  0.753318514\n52    1  1.321453245\n53    0  0.457129543\n54    1  2.718602284\n55    0  0.274229014\n56    1  2.866470604\n57    0 -1.048752804\n58    1  1.934613750\n59    0  0.623854244\n60    1  1.565941569\n61    0  0.879639483\n62    1  0.847676547\n63    0  0.166792616\n64    1  0.331424617\n65    0 -0.571791226\n66    1  1.653528641\n67    0  0.948209779\n68    1  1.403004227\n69    0  1.422267468\n70    1  3.400084686\n71    0  0.008968834\n72    1 -0.959168876\n73    0  1.505738524\n74    1  0.640799237\n75    0 -0.188008616\n76    1  2.375571370\n77    0  0.215226993\n78    1  0.129282288\n79    0  0.681303480\n80    1  1.211108638\n81    0  0.505764186\n82    1  1.735280401\n83    0  0.129339968\n84    1  1.994376549\n85    0  0.279513438\n86    1  1.681781964\n87    0  1.596839013\n88    1  1.785181491\n89    0  0.174068414\n90    1  2.498807618\n91    0  1.493503856\n92    1  1.898396960\n93    0  0.738731735\n94    1  0.722093924\n95    0  1.860652449\n96    1  0.749740413\n97    0  2.687332993\n98    1  2.882610626\n99    0  0.264299641\n100   1  0.323579100\n\n\n\n\n\nCode\nggplot(d2.f , aes(x=x.2, y=y.1)) +\n    geom_point(size = 2) +\n  geom_smooth(method = lm, se = FALSE) \n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nCode\nggplot(d2.f,\n       aes(x = as.factor(x.2),\n           y = y.1)) +\n  geom_violin(aes(fill = as.factor(x.2)),\n              alpha = .3,\n              show.legend = FALSE) +\n  geom_boxplot(aes(color = as.factor(x.2)),\n               fill = \"white\",\n               width = .2) +\n  geom_jitter(aes(color = as.factor(x.2))) +\n  labs(x = \"Treatment\",\n       y = \"y\")"
  },
  {
    "objectID": "2-Basics.html#what-do-these-visualizations-have-in-common",
    "href": "2-Basics.html#what-do-these-visualizations-have-in-common",
    "title": "GLM basics",
    "section": "What do these visualizations have in common?",
    "text": "What do these visualizations have in common?\n. . .\nLINES!\nMost of what we are going to do is represent the relationship between variables with lines (or planes or hyperplanes once we get into 2 or more variables)"
  },
  {
    "objectID": "2-Basics.html#thinking-in-terms-of-models-1",
    "href": "2-Basics.html#thinking-in-terms-of-models-1",
    "title": "GLM basics",
    "section": "Thinking in terms of models",
    "text": "Thinking in terms of models\n\nModels help us draw the lines\nOur DV (here forth Y) is what we are trying to understand\nWe hypothesize it has some relationship with your IV(s) (hence forth Xs), with what is left over described as error (E)\n\n\\(y = b_0 + b_{1}X + e\\)\n\n\\(b_{1}\\) describes the strength of association i.e.¬†the line!"
  },
  {
    "objectID": "2-Basics.html#see-this-in-our-r-code",
    "href": "2-Basics.html#see-this-in-our-r-code",
    "title": "GLM basics",
    "section": "See this in our R code",
    "text": "See this in our R code\nIndependent samples t-test\n\n\nCode\nt.1 &lt;- t.test(y.1 ~ x.2, data = d.2) \nt.1\n\n\n\n    Welch Two Sample t-test\n\ndata:  y.1 by x.2\nt = -4.4144, df = 93.846, p-value = 2.706e-05\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -1.1740658 -0.4455722\nsample estimates:\nmean in group 0 mean in group 1 \n      0.6104964       1.4203154 \n\n\n\nOne-way ANOVA\n\n\nCode\na.1 &lt;- aov(y.1 ~ x.2, data=d.2)\nsummary(a.1)\n\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nx.2          1  16.40  16.395   19.49 2.61e-05 ***\nResiduals   98  82.45   0.841                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nregression model\n\n\nCode\nlm.1 &lt;- lm(y.1 ~ x.2, data=d.2)\nsummary(lm.1)\n\n\n\nCall:\nlm(formula = y.1 ~ x.2, data = d.2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.37948 -0.57986 -0.00856  0.59773  2.09864 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   0.6105     0.1297   4.706 8.29e-06 ***\nx.2           0.8098     0.1834   4.414 2.61e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9172 on 98 degrees of freedom\nMultiple R-squared:  0.1659,    Adjusted R-squared:  0.1574 \nF-statistic: 19.49 on 1 and 98 DF,  p-value: 2.607e-05"
  },
  {
    "objectID": "2-Basics.html#comparison-across-these-three-models",
    "href": "2-Basics.html#comparison-across-these-three-models",
    "title": "GLM basics",
    "section": "Comparison across these three models",
    "text": "Comparison across these three models\n\nNote that each of these three models (t, anova, regression) were exactly the same in terms of the mode: Y ~ X\nThat is because they are the same model! Different terms referring to the same thing is one of the major stumbling blocks of stats.\nYet they gave us different information. Depending on what you are interested in some information may be more pertinent.\nWe will focus on the regression model (glm) as it is most flexible"
  },
  {
    "objectID": "2-Basics.html#general-linear-model-glm",
    "href": "2-Basics.html#general-linear-model-glm",
    "title": "GLM basics",
    "section": "General linear model (GLM)",
    "text": "General linear model (GLM)\n\nThis model (equation) can be very simple as in a treatment/control experiment\nIt can be very complex in terms of trying to understand something like academic achievement\nThe majority of our models fall under the umbrella of a general(ized) linear model (often referred to as regression models)\nModels imply our theory about how the data are generated (ie how the world works)"
  },
  {
    "objectID": "2-Basics.html#regression-equation",
    "href": "2-Basics.html#regression-equation",
    "title": "GLM basics",
    "section": "Regression Equation",
    "text": "Regression Equation\n\\[Y_i = b_{0} + b_{1}X_i +e_i\\]\n\n\\(Y_i \\sim Normal(\\mu, \\sigma)\\)\nThe DV, \\(Y\\) for each person \\(i\\) is distributed normaly, with a mean of \\(\\mu\\) and a standard deviation of \\(\\sigma\\)"
  },
  {
    "objectID": "2-Basics.html#regression-terms",
    "href": "2-Basics.html#regression-terms",
    "title": "GLM basics",
    "section": "Regression terms",
    "text": "Regression terms\n\nY / DV / Outcome / Response / Criterion\nX / IV / Predictor / Explanatory variable\nRegression coefficient (weight) / b / b* / \\(\\beta\\)\nIntercept \\(b_0\\) / \\(\\beta_{0}\\)\nError / Residuals \\(e\\)\nPredictions \\(\\hat{Y}\\)"
  },
  {
    "objectID": "2-Basics.html#regression-models",
    "href": "2-Basics.html#regression-models",
    "title": "GLM basics",
    "section": "Regression models",
    "text": "Regression models\n\nThese models are a way to convey the relationship between two (or more) variables. They translate our hypotheses into math.\nWe can use these models to get information we may be interested in (e.g.¬†means, SEs) and test hypotheses about the relationship among variables\n‚ÄúAll models are wrong but some are useful (and some are better than others)‚Äù - George Box"
  },
  {
    "objectID": "2-Basics.html#another-example",
    "href": "2-Basics.html#another-example",
    "title": "GLM basics",
    "section": "Another example",
    "text": "Another example\n\n\nCode\nlibrary(tidyverse)\nlibrary(readr)\nexample.data &lt;- read_csv(\"exampleData.csv\")\n\n\nRows: 280 Columns: 3\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\ndbl (3): id, tx, traffic.risk\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nexample.data &lt;- na.omit(example.data)\nexample.data\n\n\n# A tibble: 270 √ó 3\n      id    tx traffic.risk\n   &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt;\n 1     1     1         1.86\n 2     2     1         1   \n 3     3     1         3.29\n 4     4     1         2   \n 5     5     1         2.43\n 6     6     1         3.29\n 7     7     0         1.17\n 8     8     0         2.43\n 9     9     0         3   \n10    10     1         1.71\n# ‚Ñπ 260 more rows\n\n\n\n\n\nCode\nggplot(example.data,\n       aes(x = as.factor(tx),\n           y = traffic.risk)) +\n  geom_violin(aes(fill = as.factor(tx)),\n              alpha = .3,\n              show.legend = FALSE) +\n  geom_boxplot(aes(color = as.factor(tx)),\n               fill = \"white\",\n               width = .2) +\n  geom_jitter(aes(color = as.factor(tx))) +\n  labs(x = \"Treatment\",\n       y = \"Traffic Risk\")\n\n\n\n\n\n\n\n\nCode\nt.test(traffic.risk ~ tx, data = example.data, \n              var.equal = TRUE) \n\n\n\n    Two Sample t-test\n\ndata:  traffic.risk by tx\nt = 4.9394, df = 268, p-value = 1.381e-06\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n 0.2893360 0.6728755\nsample estimates:\nmean in group 0 mean in group 1 \n       2.650641        2.169535 \n\n\n\n\n\nCode\na.1 &lt;- aov(traffic.risk ~ tx, data = example.data) \nsummary(a.1)\n\n\n             Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ntx            1   14.8  14.800    24.4 1.38e-06 ***\nResiduals   268  162.6   0.607                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nCode\nmod.1 &lt;- lm(traffic.risk ~ tx, data = example.data)\nsummary(mod.1)\n\n\n\nCall:\nlm(formula = traffic.risk ~ tx, data = example.data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.65064 -0.59811 -0.02668  0.54475  2.54475 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.65064    0.07637  34.707  &lt; 2e-16 ***\ntx          -0.48111    0.09740  -4.939 1.38e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7789 on 268 degrees of freedom\nMultiple R-squared:  0.08344,   Adjusted R-squared:  0.08002 \nF-statistic:  24.4 on 1 and 268 DF,  p-value: 1.381e-06\n\n\n\n\n\nCode\nmod.1 &lt;- lm(traffic.risk ~ tx, data = example.data)\nanova(mod.1)\n\n\nAnalysis of Variance Table\n\nResponse: traffic.risk\n           Df Sum Sq Mean Sq F value    Pr(&gt;F)    \ntx          1  14.80 14.7999  24.398 1.381e-06 ***\nResiduals 268 162.57  0.6066                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "2-Basics.html#example-summary",
    "href": "2-Basics.html#example-summary",
    "title": "GLM basics",
    "section": "Example summary",
    "text": "Example summary\n\nSame p-values for each test; same SS; same test!\nt-test is a special form of a linear model\nanova is a special form of a linear model\nBecause the anova and t-test are narrower models, we will be working with the general linear model"
  },
  {
    "objectID": "2-Basics.html#parts-of-the-model",
    "href": "2-Basics.html#parts-of-the-model",
    "title": "GLM basics",
    "section": "Parts of the model",
    "text": "Parts of the model\n\\[Y_i = b_{0} + b_{1}X_i + e_i\\] \\[T.risk_i = b_{0} + b_{1}TX_i + e_i\\]\n\nEach individual has a unique Y value an X value and a residual/error term\n\nThe model only has a single \\(b_{0}\\) and \\(b_{1}\\) term. These are the regression parameters. \\(b_{0}\\) is the intercept and \\(b_{1}\\) quantifies the relationship between your model of the world and the DV."
  },
  {
    "objectID": "2-Basics.html#what-do-the-estimates-tell-us",
    "href": "2-Basics.html#what-do-the-estimates-tell-us",
    "title": "GLM basics",
    "section": "What do the estimates tell us?",
    "text": "What do the estimates tell us?\n\n\nCode\nmod.1 &lt;- lm(traffic.risk ~ tx, data = example.data)\nlibrary(broom)\ntidy(mod.1)\n\n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    2.65     0.0764     34.7  3.80e-101\n2 tx            -0.481    0.0974     -4.94 1.38e-  6\n\n\n\n\n\nCode\nexample.data %&gt;% \n  group_by(tx) %&gt;% \n  summarise(mean(traffic.risk))\n\n\n# A tibble: 2 √ó 2\n     tx `mean(traffic.risk)`\n  &lt;dbl&gt;                &lt;dbl&gt;\n1     0                 2.65\n2     1                 2.17"
  },
  {
    "objectID": "2-Basics.html#how-to-interpret-regression-estimates",
    "href": "2-Basics.html#how-to-interpret-regression-estimates",
    "title": "GLM basics",
    "section": "How to interpret regression estimates",
    "text": "How to interpret regression estimates\n\nIntercept is the mean of group of variable tx that is coded 0\nRegression coefficient is the slope or rise over run, scaled as a 1 unit on the x axis\n‚ÄúFor a one unit change in X, there is a b1 predicted change in Y.‚Äù\nRegression coefficient is the difference in means between the groups, given that we coded our groups as 0 and 1.\n\n\n\n\nCode\nlibrary(ggstatsplot)\n\n\nYou can cite this package as:\n     Patil, I. (2021). Visualizations with statistical details: The 'ggstatsplot' approach.\n     Journal of Open Source Software, 6(61), 3167, doi:10.21105/joss.03167\n\n\nCode\nggstatsplot::ggbetweenstats(\n  data = example.data,\n  x = tx,\n  y = traffic.risk,\n  ylab = \"Traffic Risk Score\", # label for the y-axis variable\n  xlab = \"Treatment group\", # label for the x-axis variable\n  bf.message = FALSE, \n  messages = FALSE\n) \n\n\n\n\n\n\nNote that the same interpretation for a regression line holds: for a 1 unit change in X (tx) there is a predicted b change in Y (traffic risk)\n\n\nCode\nlibrary(ggplot2)\nggplot(example.data, aes(x=tx, y=traffic.risk)) +\n    geom_point() +    \n    geom_smooth(method=lm,   # Add linear regression line\n                se=FALSE)    # Don't add shaded confidence region\n\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "2-Basics.html#how-to-interpret-regression-estimates-1",
    "href": "2-Basics.html#how-to-interpret-regression-estimates-1",
    "title": "GLM basics",
    "section": "How to interpret regression estimates",
    "text": "How to interpret regression estimates\n\nThe entire class will go over different ways to interpret these estimates/parameters/coefficients\nIntercept (b0) signifies the level of Y when your model IVs (Xs) are zero\nRegression (b1) signifies the difference for a one unit change in your X"
  },
  {
    "objectID": "2-Basics.html#standard-errors",
    "href": "2-Basics.html#standard-errors",
    "title": "GLM basics",
    "section": "Standard errors",
    "text": "Standard errors\n\nThese coefficients are ‚Äúbest guesses‚Äù at some population parameter we want to make inferences about.\nTo do so we must balance our signal to our noise. If we have a strong signal (steep regression line/difference between groups) that would imply the groups differ.\nIf there was a lot of noise in that assessment then a big difference between groups may not be meaningful. We assess this ‚Äúnoise‚Äù component with our standard errors"
  },
  {
    "objectID": "2-Basics.html#sampling-distribution-refresher",
    "href": "2-Basics.html#sampling-distribution-refresher",
    "title": "GLM basics",
    "section": "Sampling distribution refresher",
    "text": "Sampling distribution refresher\n\nWe collect a sample and calculate a sample statistic \\(b_1\\)\nThis statistic is not a perfect assessment of the population\nWe calculate a sampling distribution to represent all possible samples we could have gotten from the same population with the same sample size\nThe standard deviation of the sampling distribution (standard error) reflects the spread of the hypothetical scores we could have gotten\nA large standard error means we have a flat sampling distribution and thus should not trust the estimate.\nPer convention, if our estimate is &gt; 2xSE away from 0, we say our estimate is ‚Äúsignificantly different from zero‚Äù"
  },
  {
    "objectID": "2-Basics.html#predicted-scores",
    "href": "2-Basics.html#predicted-scores",
    "title": "GLM basics",
    "section": "Predicted scores",
    "text": "Predicted scores\n\nBased on the output how do I calculate means for each group?\n\n\ntidy(mod.1)\n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    2.65     0.0764     34.7  3.80e-101\n2 tx            -0.481    0.0974     -4.94 1.38e-  6"
  },
  {
    "objectID": "2-Basics.html#anova-as-regression",
    "href": "2-Basics.html#anova-as-regression",
    "title": "GLM basics",
    "section": "ANOVA as regression",
    "text": "ANOVA as regression\n\n‚ÄúFor a one unit change in X, there is a b1 predicted change in Y‚Äù will always be true.\nNominal/categorical variables do not have any inherent numbers associated with them so we need to assign them numbers\nWhat numbers you assign will impact the equation/estimates/hypothesis you can test\n\nBehoove you to code them as useful numbers. O and 1 are useful and are the default in R."
  },
  {
    "objectID": "2-Basics.html#anova-as-regression-1",
    "href": "2-Basics.html#anova-as-regression-1",
    "title": "GLM basics",
    "section": "ANOVA as regression",
    "text": "ANOVA as regression\n\\[T.risk_i = b_{0} + b_{1}TX_i + e_i\\]\n\n\nCode\nexample.data\n\n\n# A tibble: 270 √ó 3\n      id    tx traffic.risk\n   &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt;\n 1     1     1         1.86\n 2     2     1         1   \n 3     3     1         3.29\n 4     4     1         2   \n 5     5     1         2.43\n 6     6     1         3.29\n 7     7     0         1.17\n 8     8     0         2.43\n 9     9     0         3   \n10    10     1         1.71\n# ‚Ñπ 260 more rows\n\n\n\n\n\nCode\nlibrary(dplyr)\nexample.data$tx.r &lt;- as.factor(example.data$tx)\nexample.data$tx.r &lt;- recode_factor(example.data$tx.r, \"0\" = \"control\", \"1\" = \"treatment\") \n\n\nCreate a new variable that is not numeric\n\n\nCode\nexample.data\n\n\n# A tibble: 270 √ó 4\n      id    tx traffic.risk tx.r     \n   &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt; &lt;fct&gt;    \n 1     1     1         1.86 treatment\n 2     2     1         1    treatment\n 3     3     1         3.29 treatment\n 4     4     1         2    treatment\n 5     5     1         2.43 treatment\n 6     6     1         3.29 treatment\n 7     7     0         1.17 control  \n 8     8     0         2.43 control  \n 9     9     0         3    control  \n10    10     1         1.71 treatment\n# ‚Ñπ 260 more rows\n\n\n\n\n\nCode\nmod.1 &lt;- lm(traffic.risk ~ tx, data = example.data)\ntidy(mod.1)\n\n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    2.65     0.0764     34.7  3.80e-101\n2 tx            -0.481    0.0974     -4.94 1.38e-  6\n\n\n\n\nCode\nmod.1r &lt;- lm(traffic.risk ~ tx.r, data = example.data)\ntidy(mod.1r)\n\n\n# A tibble: 2 √ó 5\n  term          estimate std.error statistic   p.value\n  &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)      2.65     0.0764     34.7  3.80e-101\n2 tx.rtreatment   -0.481    0.0974     -4.94 1.38e-  6"
  },
  {
    "objectID": "2-Basics.html#what-if-we-changed-0-and-1-to-other-values",
    "href": "2-Basics.html#what-if-we-changed-0-and-1-to-other-values",
    "title": "GLM basics",
    "section": "What if we changed 0 and 1 to other values?",
    "text": "What if we changed 0 and 1 to other values?\n\nInfinite number of ways to code categorical/nominal variables, only a few meaningful ways\n\nThe R default is called ‚Äúdummy coding‚Äù\n\nUses 0s and 1s to put numbers to categories. We will soon see what this looks like when you have more than 2 groups.\nChanging the numbers changes‚Ä¶?"
  },
  {
    "objectID": "2-Basics.html#effect-coding",
    "href": "2-Basics.html#effect-coding",
    "title": "GLM basics",
    "section": "Effect coding",
    "text": "Effect coding\n\n\nCode\nexample.data$tx.effect &lt;- dplyr::recode(example.data$tx, '0' = -1, '1' = 1) \n\n\n\n\nCode\nexample.data\n\n\n# A tibble: 270 √ó 5\n      id    tx traffic.risk tx.r      tx.effect\n   &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt; &lt;fct&gt;         &lt;dbl&gt;\n 1     1     1         1.86 treatment         1\n 2     2     1         1    treatment         1\n 3     3     1         3.29 treatment         1\n 4     4     1         2    treatment         1\n 5     5     1         2.43 treatment         1\n 6     6     1         3.29 treatment         1\n 7     7     0         1.17 control          -1\n 8     8     0         2.43 control          -1\n 9     9     0         3    control          -1\n10    10     1         1.71 treatment         1\n# ‚Ñπ 260 more rows"
  },
  {
    "objectID": "2-Basics.html#effect-coding-1",
    "href": "2-Basics.html#effect-coding-1",
    "title": "GLM basics",
    "section": "Effect coding",
    "text": "Effect coding\n\n\nCode\nmod.1.eff &lt;- lm(traffic.risk ~ tx.effect, data = example.data)\ntidy(mod.1.eff)\n\n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    2.41     0.0487     49.5  8.15e-137\n2 tx.effect     -0.241    0.0487     -4.94 1.38e-  6\n\n\n\nsystematically changes both the intercept and the regression estimate\n\n\n\n\nCode\neffect &lt;- tidy(mod.1.eff)\neffect\n\n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    2.41     0.0487     49.5  8.15e-137\n2 tx.effect     -0.241    0.0487     -4.94 1.38e-  6\n\n\n\n\nCode\ndummy &lt;- tidy(mod.1)\ndummy\n\n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    2.65     0.0764     34.7  3.80e-101\n2 tx            -0.481    0.0974     -4.94 1.38e-  6\n\n\n\nIntercept: value when your predictor X is zero\nRegression coefficient: one unit increase in X is associated with a (regression estimate) predicted increase in Y"
  },
  {
    "objectID": "2-Basics.html#effect-coding-2",
    "href": "2-Basics.html#effect-coding-2",
    "title": "GLM basics",
    "section": "Effect coding",
    "text": "Effect coding\nConsists of -1, 1s (And zeros for more than 2 groups)\n\nThe intercept is the ‚Äúgrand mean‚Äù or ‚Äúmean of means‚Äù if unbalanced\nThe regression coefficient represents the group ‚Äúeffect‚Äù ie the difference between the grand mean and the group labeled 1 (we will revisit this when we have more than 2 groups as it will make more sense)\n\n\nCommon to use for Factorial ANOVA designs"
  },
  {
    "objectID": "2-Basics.html#dummy-coding",
    "href": "2-Basics.html#dummy-coding",
    "title": "GLM basics",
    "section": "Dummy coding",
    "text": "Dummy coding\n\nMore appropriate when you are interested in comparing to a specific group rather than an ‚Äúaverage person.‚Äù\nIntercept: value of the group coded zero\nRegression coefficient: mean difference between groups"
  },
  {
    "objectID": "2-Basics.html#contrast-coding",
    "href": "2-Basics.html#contrast-coding",
    "title": "GLM basics",
    "section": "Contrast coding",
    "text": "Contrast coding\n\nAs our models get more complex our coding schemes can too\nWhat happens if you code the groups -.5 and .5?\nThese make more sense when we have more groups. More groups require more independent variables, however."
  },
  {
    "objectID": "2-Basics.html#categorical-coding-summary",
    "href": "2-Basics.html#categorical-coding-summary",
    "title": "GLM basics",
    "section": "categorical coding summary",
    "text": "categorical coding summary\n\nIn the end, it really doesn‚Äôt matter how you code your model. The overall ‚Äúfit‚Äù of the model will be exactly the same because it is the same model.\nThe only thing that changes is the interpretation of your coefficients.\nEven then, you can recreate any test you want regardless of coding scheme. As a result, we often leave the default coding in place."
  },
  {
    "objectID": "2-Basics.html#statistical-inference",
    "href": "2-Basics.html#statistical-inference",
    "title": "GLM basics",
    "section": "Statistical Inference",
    "text": "Statistical Inference\n\nThe way the world is = our model + error\nHow good is our model? Is it a good representation of reality? Does it ‚Äúfit‚Äù the data well?\nNeed to go beyond asking if it is significant, because what does that mean? Remember, all models are wrong\nWe are going to make predictions and see if the predictions (based on our model) matches our data\nWe can then compare one model to another to see which one matches our data better ie which one is a better representation of reality."
  },
  {
    "objectID": "2-Basics.html#predictions",
    "href": "2-Basics.html#predictions",
    "title": "GLM basics",
    "section": "Predictions",
    "text": "Predictions\n\nOur model is a prediction machine.\nThey are created by simply plugging a persons Xs into the created model\nIf you have bs and have Xs you can create a prediction\n\n\\(\\hat{Y}_{i}\\) = 2.65064 + -0.48111* \\(X_{i}\\)"
  },
  {
    "objectID": "2-Basics.html#predictions-1",
    "href": "2-Basics.html#predictions-1",
    "title": "GLM basics",
    "section": "Predictions",
    "text": "Predictions\n\nWe want our predictions to be close to our actual data for each person ( \\(Y_{i}\\) )\nThe difference between the actual data and our our prediction ( \\(Y_{i} - \\hat{Y}_{i} = e\\) ) is the residual, how far we are ‚Äúoff‚Äù. This tells us how good our fit is.\nYou can have the same estimates for two models but completely different fit.\nPreviously you may have evaluated overall model fit by looking at Eta Squared, SS Error and visualizing observations around group means"
  },
  {
    "objectID": "2-Basics.html#which-one-has-better-fit",
    "href": "2-Basics.html#which-one-has-better-fit",
    "title": "GLM basics",
    "section": "Which one has better fit?",
    "text": "Which one has better fit?\n\nCan you point out the predictions?\n\n\n\nCode\ntwogroup_fun = function(nrep = 100, b0 = 6, b1 = -2, sigma = 1) {\n     ngroup = 2\n     group = rep( c(\"group1\", \"group2\"), each = nrep)\n     eps = rnorm(ngroup*nrep, 0, sigma)\n     traffic = b0 + b1*(group == \"group2\") + eps\n     growthfit = lm(traffic ~ group)\n     growthfit\n}\n\n\ntwogroup_fun2 = function(nrep = 100, b0 = 6, b1 = -2, sigma = 2) {\n     ngroup = 2\n     group = rep( c(\"group1\", \"group2\"), each = nrep)\n     eps = rnorm(ngroup*nrep, 0, sigma)\n     traffic = b0 + b1*(group == \"group2\") + eps\n     growthfit = lm(traffic ~ group)\n     growthfit\n}\n\nset.seed(16)\nlibrary(broom)\nlm1 &lt;- augment(twogroup_fun())\n\nset.seed(16)\nlm2 &lt;- augment(twogroup_fun2())\n\nplot1&lt;- ggplot(lm1) +\n  aes(x = group, y = traffic) +\n  geom_violin() + geom_boxplot() + geom_jitter() + ylim(-1, 11)\n\nplot2&lt;- ggplot(lm2) +\n  aes(x = group, y = traffic) +\n  geom_violin() + geom_boxplot() + geom_jitter() + ylim(-1, 11)\n\n\nlibrary(gridExtra)\n\n\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\n\nCode\n grid.arrange(plot1, plot2, ncol=2)\n\n\nWarning: Removed 1 row containing non-finite outside the scale range\n(`stat_ydensity()`).\n\n\nWarning: Removed 1 row containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`)."
  },
  {
    "objectID": "2-Basics.html#easy-to-examine-fit-with-lm-objects",
    "href": "2-Basics.html#easy-to-examine-fit-with-lm-objects",
    "title": "GLM basics",
    "section": "Easy to examine fit with lm objects",
    "text": "Easy to examine fit with lm objects\n\nThese are automatically created anytime you run a lm in R\n\n\n\nCode\nmod.1 &lt;- lm(traffic.risk ~ tx, data = example.data)\n\n\n\ncoefficients(mod.1)       # coefficients\nresiduals(mod.1)          # residuals\nfitted.values(mod.1)      # fitted values ie predicted\nsummary(mod.1)$r.squared  # R-sq for the model\nsummary(mod.1)$sigma      # sd of residuals\n\n\n\ncoefficients(mod.1)\n\n(Intercept)          tx \n  2.6506410  -0.4811057 \n\n\n\n\nfitted.values(mod.1)\n\n       1        2        3        4        5        6        7        8 \n2.169535 2.169535 2.169535 2.169535 2.169535 2.169535 2.650641 2.650641 \n       9       10       11       12       13       14       15       16 \n2.650641 2.169535 2.169535 2.650641 2.650641 2.650641 2.650641 2.650641 \n      17       18       19       20       21       22       23       24 \n2.650641 2.650641 2.650641 2.169535 2.169535 2.169535 2.169535 2.169535 \n      25       26       27       28       29       30       31       32 \n2.650641 2.650641 2.650641 2.650641 2.650641 2.650641 2.650641 2.650641 \n      33       34       35       36       37       38       39       40 \n2.650641 2.650641 2.650641 2.650641 2.169535 2.169535 2.169535 2.169535 \n      41       42       43       44       45       46       47       48 \n2.650641 2.169535 2.650641 2.650641 2.169535 2.650641 2.650641 2.650641 \n      49       50       51       52       53       54       55       56 \n2.169535 2.650641 2.169535 2.169535 2.169535 2.169535 2.169535 2.650641 \n      57       58       59       60       61       62       63       64 \n2.169535 2.169535 2.169535 2.169535 2.650641 2.169535 2.169535 2.650641 \n      65       66       67       68       69       70       71       72 \n2.169535 2.169535 2.650641 2.650641 2.169535 2.169535 2.169535 2.169535 \n      73       74       75       76       77       78       79       80 \n2.169535 2.169535 2.169535 2.169535 2.650641 2.169535 2.169535 2.650641 \n      81       82       83       84       85       86       87       88 \n2.169535 2.650641 2.650641 2.650641 2.650641 2.169535 2.169535 2.650641 \n      89       90       91       92       93       94       95       96 \n2.169535 2.169535 2.169535 2.650641 2.169535 2.169535 2.169535 2.169535 \n      97       98       99      100      101      102      103      104 \n2.169535 2.169535 2.169535 2.169535 2.169535 2.169535 2.169535 2.169535 \n     105      106      107      108      109      110      111      112 \n2.650641 2.650641 2.650641 2.650641 2.650641 2.650641 2.650641 2.650641 \n     113      114      115      116      117      118      119      120 \n2.650641 2.650641 2.650641 2.650641 2.650641 2.650641 2.169535 2.650641 \n     121      122      123      124      125      126      127      128 \n2.650641 2.650641 2.650641 2.169535 2.169535 2.169535 2.169535 2.169535 \n     129      130      131      132      133      134      135      136 \n2.650641 2.169535 2.169535 2.169535 2.169535 2.169535 2.169535 2.169535 \n     137      138      139      140      141      142      143      144 \n2.169535 2.169535 2.169535 2.169535 2.169535 2.169535 2.169535 2.169535 \n     145      146      147      148      149      150      151      152 \n2.169535 2.169535 2.169535 2.169535 2.650641 2.650641 2.650641 2.650641 \n     153      154      155      156      157      158      159      160 \n2.650641 2.650641 2.169535 2.650641 2.169535 2.169535 2.169535 2.169535 \n     161      162      163      164      165      166      167      168 \n2.650641 2.169535 2.169535 2.169535 2.169535 2.169535 2.169535 2.169535 \n     169      170      171      172      173      174      175      176 \n2.169535 2.169535 2.650641 2.650641 2.169535 2.169535 2.169535 2.650641 \n     177      178      179      180      181      182      183      184 \n2.650641 2.650641 2.169535 2.169535 2.169535 2.169535 2.169535 2.650641 \n     185      186      187      188      189      190      191      192 \n2.169535 2.169535 2.169535 2.169535 2.169535 2.650641 2.169535 2.169535 \n     193      194      195      196      197      198      199      200 \n2.169535 2.650641 2.169535 2.169535 2.650641 2.169535 2.169535 2.169535 \n     201      202      203      204      205      206      207      208 \n2.169535 2.650641 2.169535 2.169535 2.169535 2.169535 2.169535 2.169535 \n     209      210      211      212      213      214      215      216 \n2.169535 2.169535 2.169535 2.169535 2.169535 2.650641 2.650641 2.169535 \n     217      218      219      220      221      222      223      224 \n2.169535 2.169535 2.169535 2.169535 2.169535 2.169535 2.650641 2.169535 \n     225      226      227      228      229      230      231      232 \n2.169535 2.169535 2.169535 2.650641 2.169535 2.169535 2.650641 2.169535 \n     233      234      235      236      237      238      239      240 \n2.169535 2.650641 2.169535 2.169535 2.169535 2.169535 2.169535 2.650641 \n     241      242      243      244      245      246      247      248 \n2.650641 2.169535 2.650641 2.169535 2.169535 2.169535 2.169535 2.650641 \n     249      250      251      252      253      254      255      256 \n2.650641 2.650641 2.169535 2.650641 2.650641 2.169535 2.169535 2.169535 \n     257      258      259      260      261      262      263      264 \n2.169535 2.169535 2.650641 2.169535 2.169535 2.650641 2.650641 2.650641 \n     265      266      267      268      269      270 \n2.650641 2.650641 2.650641 2.650641 2.650641 2.650641 \n\n\n\n\nresiduals(mod.1)\n\n           1            2            3            4            5            6 \n-0.312392427 -1.169535284  1.116179002 -0.169535284  0.259036145  1.116179002 \n           7            8            9           10           11           12 \n-1.483974359 -0.222069597  0.349358974 -0.455249570  1.687607573 -0.079212455 \n          13           14           15           16           17           18 \n 0.206501831  0.920787545  1.349358974  0.349358974 -0.936355312 -1.364926740 \n          19           20           21           22           23           24 \n-0.364926740 -0.312392427 -0.598106713 -0.026678141  0.116179002 -0.455249570 \n          25           26           27           28           29           30 \n 1.063644688 -0.650641026 -0.079212455  0.206501831  0.777930403 -1.079212455 \n          31           32           33           34           35           36 \n 0.063644688 -0.507783883  0.777930403 -0.936355312 -1.079212455 -1.079212455 \n          37           38           39           40           41           42 \n 0.116179002 -0.598106713 -0.598106713  0.116179002  0.635073260 -0.598106713 \n          43           44           45           46           47           48 \n-0.507783883  0.635073260  0.116179002 -0.793498169  0.635073260  1.349358974 \n          49           50           51           52           53           54 \n 0.830464716  0.492216117  0.497131383 -0.598106713 -0.312392427 -0.883820998 \n          55           56           57           58           59           60 \n 0.973321859 -0.222069597 -1.169535284  0.259036145 -0.455249570 -0.169535284 \n          61           62           63           64           65           66 \n-0.079212455  0.544750430  1.259036145  0.920787545  0.830464716  2.259036145 \n          67           68           69           70           71           72 \n-0.650641026 -1.650641026 -0.169535284  1.687607573 -1.026678141  0.401893287 \n          73           74           75           76           77           78 \n 0.973321859 -1.026678141 -0.598106713  0.973321859  0.849358974 -0.598106713 \n          79           80           81           82           83           84 \n 0.973321859  0.777930403  1.401893287 -1.364926740 -1.079212455 -0.222069597 \n          85           86           87           88           89           90 \n 0.492216117  1.687607573 -1.026678141 -0.650641026 -0.312392427 -0.312392427 \n          91           92           93           94           95           96 \n 1.973321859  0.920787545 -1.169535284  0.116179002 -1.026678141  0.973321859 \n          97           98           99          100          101          102 \n-0.026678141 -0.598106713 -0.883820998  1.116179002 -0.883820998 -0.169535284 \n         103          104          105          106          107          108 \n-1.026678141  0.116179002  0.063644688  0.349358974  0.206501831  0.349358974 \n         109          110          111          112          113          114 \n 1.206501831  0.777930403  0.777930403 -0.079212455  1.206501831  0.349358974 \n         115          116          117          118          119          120 \n 0.492216117  0.777930403 -0.650641026  0.206501831  0.544750430  0.920787545 \n         121          122          123          124          125          126 \n-0.222069597  0.349358974 -0.793498169  0.973321859 -0.026678141  1.544750430 \n         127          128          129          130          131          132 \n 0.259036145 -0.883820998 -1.079212455 -1.026678141 -0.598106713 -0.169535284 \n         133          134          135          136          137          138 \n 0.259036145 -1.169535284  1.259036145 -0.740963855 -0.169535284 -0.169535284 \n         139          140          141          142          143          144 \n 0.116179002  1.116179002 -0.026678141 -0.502868617 -0.312392427  1.259036145 \n         145          146          147          148          149          150 \n 2.544750430 -1.026678141  0.401893287  0.830464716  0.920787545 -1.507783883 \n         151          152          153          154          155          156 \n 0.920787545 -1.364926740 -0.222069597  0.063644688 -0.312392427  0.349358974 \n         157          158          159          160          161          162 \n 0.401893287 -0.883820998 -0.455249570 -0.312392427  0.349358974 -1.026678141 \n         163          164          165          166          167          168 \n 0.687607573 -0.169535284  1.116179002  0.116179002 -0.598106713 -0.669535284 \n         169          170          171          172          173          174 \n 0.259036145 -0.455249570  0.492216117 -0.936355312 -0.598106713  0.687607573 \n         175          176          177          178          179          180 \n 0.401893287  0.492216117  0.920787545 -0.364926740 -0.026678141 -0.740963855 \n         181          182          183          184          185          186 \n-0.026678141 -0.026678141  0.259036145 -1.364926740 -0.598106713 -0.598106713 \n         187          188          189          190          191          192 \n-1.026678141 -1.169535284 -0.002868617 -0.936355312 -0.740963855  0.687607573 \n         193          194          195          196          197          198 \n-0.598106713  1.349358974 -0.883820998  0.259036145 -0.364926740 -0.740963855 \n         199          200          201          202          203          204 \n 0.401893287 -0.883820998 -0.312392427 -0.079212455  0.401893287  0.259036145 \n         205          206          207          208          209          210 \n 0.687607573 -0.740963855 -0.598106713 -0.455249570  0.259036145  0.259036145 \n         211          212          213          214          215          216 \n 0.401893287 -0.740963855 -0.312392427  0.635073260 -1.507783883 -0.598106713 \n         217          218          219          220          221          222 \n 0.973321859 -0.312392427  0.116179002 -0.026678141  1.259036145 -0.169535284 \n         223          224          225          226          227          228 \n-1.079212455  0.830464716 -0.883820998 -0.026678141  0.401893287  0.635073260 \n         229          230          231          232          233          234 \n-0.740963855  0.116179002 -0.793498169 -0.598106713 -0.026678141  1.492216117 \n         235          236          237          238          239          240 \n 0.544750430  0.544750430 -0.312392427 -0.455249570 -0.455249570 -0.936355312 \n         241          242          243          244          245          246 \n 0.063644688  0.401893287 -0.650641026 -0.312392427  0.401893287  0.259036145 \n         247          248          249          250          251          252 \n-1.026678141  0.777930403  1.063644688  0.063644688  0.973321859  0.920787545 \n         253          254          255          256          257          258 \n 0.349358974  0.544750430  0.259036145  0.973321859 -0.455249570 -0.740963855 \n         259          260          261          262          263          264 \n-0.936355312 -1.169535284 -0.169535284  0.635073260 -0.222069597 -0.222069597 \n         265          266          267          268          269          270 \n-0.222069597  0.349358974 -0.364926740 -0.079212455 -0.507783883 -0.079212455"
  },
  {
    "objectID": "2-Basics.html#an-aside-concerning-lm-objects",
    "href": "2-Basics.html#an-aside-concerning-lm-objects",
    "title": "GLM basics",
    "section": "An aside concerning lm objects",
    "text": "An aside concerning lm objects\n\nlm objects consist of the information embedded in your linear model\nR often handles model objects poorly due to them not necessarily being in a usable data frame (lists!)\nthe broom package makes model objects into dataframes\n\n\n\nCode\nlibrary(broom)\nfit.1.tidy &lt;- tidy(mod.1)  \nfit.1.tidy\n\n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    2.65     0.0764     34.7  3.80e-101\n2 tx            -0.481    0.0974     -4.94 1.38e-  6\n\n\n\n\nAugment function from the broom package amends the original dataset with lm object content. The new variable names of have a ‚Äú.‚Äù in front of the name to distinguish\n\n\n\nCode\nfit.1.data &lt;- augment(mod.1) \nfit.1.data\n\n\n# A tibble: 270 √ó 8\n   traffic.risk    tx .fitted .resid    .hat .sigma  .cooksd .std.resid\n          &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n 1         1.86     1    2.17 -0.312 0.00602  0.780 0.000490     -0.402\n 2         1        1    2.17 -1.17  0.00602  0.777 0.00687      -1.51 \n 3         3.29     1    2.17  1.12  0.00602  0.777 0.00626       1.44 \n 4         2        1    2.17 -0.170 0.00602  0.780 0.000144     -0.218\n 5         2.43     1    2.17  0.259 0.00602  0.780 0.000337      0.334\n 6         3.29     1    2.17  1.12  0.00602  0.777 0.00626       1.44 \n 7         1.17     0    2.65 -1.48  0.00962  0.775 0.0178       -1.91 \n 8         2.43     0    2.65 -0.222 0.00962  0.780 0.000398     -0.287\n 9         3        0    2.65  0.349 0.00962  0.780 0.000986      0.451\n10         1.71     1    2.17 -0.455 0.00602  0.780 0.00104      -0.586\n# ‚Ñπ 260 more rows"
  },
  {
    "objectID": "2-Basics.html#statistical-inference-1",
    "href": "2-Basics.html#statistical-inference-1",
    "title": "GLM basics",
    "section": "Statistical Inference",
    "text": "Statistical Inference\n\nTo the extent that we can generate different predicted values of Y based on the values of the predictors, our model is doing well\nSaid differently, the closer our model is to the ‚Äúactual‚Äù data generating model, our guesses ( \\(\\hat{Y}\\) ) will be closer to our actual data ( \\(Y\\) )\n\n\n\n\nCode\nlibrary(ggstatsplot)\nggstatsplot::ggbetweenstats(\n  data = example.data,\n  x = tx,\n  y = traffic.risk,\n  xlab = \"Traffic Risk Score\", # label for the x-axis variable\n  ylab = \"Treatment group\", # label for the y-axis variable\n  bf.message = FALSE,\n  mean.plotting = TRUE,\n  plot.type = \"violin\",\n  messages = FALSE\n) +\n  geom_hline(yintercept = 2.35)+\n  geom_text(aes(.55,2.5,label = \"Average\"))\n\n\nWarning in geom_text(aes(0.55, 2.5, label = \"Average\")): All aesthetics have length 1, but the data has 270 rows.\n‚Ñπ Please consider using `annotate()` or provide this layer with data containing\n  a single row."
  },
  {
    "objectID": "2-Basics.html#partitioning-the-variation-in-y",
    "href": "2-Basics.html#partitioning-the-variation-in-y",
    "title": "GLM basics",
    "section": "Partitioning the variation in Y",
    "text": "Partitioning the variation in Y\n\\[ \\sum (Y_i - \\bar{Y})^2 = \\sum (\\hat{Y}_i -\\bar{Y})^2 + \\sum(Y_i - \\hat{Y}_i)^2 \\]\n\nSS total = SS between + SS within\nSS total = SS regression + SS residual (or error)"
  },
  {
    "objectID": "2-Basics.html#what-can-we-do-with-this",
    "href": "2-Basics.html#what-can-we-do-with-this",
    "title": "GLM basics",
    "section": "What can we do with this?",
    "text": "What can we do with this?\n\nomnibus F tests (ANOVA)\nWhat hypothesis does the omnibus F test test, generally?\n\n\\[s_{y}^2 = s_{regression}^2 + s_{residual}^2\\]\n\\[1 = \\frac{s_{regression}^2}{s_{y}^2} + \\frac{s_{residual}^2}{s_{y}^2}\\]"
  },
  {
    "objectID": "2-Basics.html#coefficient-of-determination",
    "href": "2-Basics.html#coefficient-of-determination",
    "title": "GLM basics",
    "section": "Coefficient of Determination",
    "text": "Coefficient of Determination\n\\[\\frac{s_{regression}^2}{s_{y}^2} = \\frac{SS_{regression}}{SS_{Y}} = R^2\\]\n\nPercent (of total) variance explained by your model‚Ä¶which currently are groups\nAnother way of asking how much variance group status explains"
  },
  {
    "objectID": "2-Basics.html#r2-and-eta-squared",
    "href": "2-Basics.html#r2-and-eta-squared",
    "title": "GLM basics",
    "section": "\\(R^2\\) and Eta squared",
    "text": "\\(R^2\\) and Eta squared\n\nsummary(mod.1)$r.squared\n\n[1] 0.08344007\n\n\n\nlibrary(lsr)\netaSquared(mod.1)\n\n       eta.sq eta.sq.part\ntx 0.08344007  0.08344007"
  },
  {
    "objectID": "2-Basics.html#r2-for-different-coding-schemes",
    "href": "2-Basics.html#r2-for-different-coding-schemes",
    "title": "GLM basics",
    "section": "\\(R^2\\) for different coding schemes",
    "text": "\\(R^2\\) for different coding schemes\n\nglance(mod.1)\n\n# A tibble: 1 √ó 12\n  r.squared adj.r.squared sigma statistic    p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    0.0834        0.0800 0.779      24.4 0.00000138     1  -315.  635.  646.\n# ‚Ñπ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\n\nglance(mod.1.eff)\n\n# A tibble: 1 √ó 12\n  r.squared adj.r.squared sigma statistic    p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    0.0834        0.0800 0.779      24.4 0.00000138     1  -315.  635.  646.\n# ‚Ñπ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;"
  },
  {
    "objectID": "2-Basics.html#note-the-r2-p-value",
    "href": "2-Basics.html#note-the-r2-p-value",
    "title": "GLM basics",
    "section": "Note the \\(R^2\\) p-value",
    "text": "Note the \\(R^2\\) p-value\n\n\nCode\nsummary(mod.1)\n\n\n\nCall:\nlm(formula = traffic.risk ~ tx, data = example.data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.65064 -0.59811 -0.02668  0.54475  2.54475 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.65064    0.07637  34.707  &lt; 2e-16 ***\ntx          -0.48111    0.09740  -4.939 1.38e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7789 on 268 degrees of freedom\nMultiple R-squared:  0.08344,   Adjusted R-squared:  0.08002 \nF-statistic:  24.4 on 1 and 268 DF,  p-value: 1.381e-06"
  },
  {
    "objectID": "2-Basics.html#summary",
    "href": "2-Basics.html#summary",
    "title": "GLM basics",
    "section": "Summary",
    "text": "Summary"
  },
  {
    "objectID": "2-Basics.html#summary-1",
    "href": "2-Basics.html#summary-1",
    "title": "GLM basics",
    "section": "Summary",
    "text": "Summary\n\nWe are using linear models to do the exact same tests as t-tests and ANOVAs\nIt is the exact same because t-tests and ANOVAs are part of the general linear model\nThe GLM provides a more systematic way at 1) building and testing your theoretical model and 2) comparing between alternative theoretical models\nYou can get 1) estimates and 2) fit statistics from the model. Both are important."
  },
  {
    "objectID": "9-logistic.html",
    "href": "9-logistic.html",
    "title": "Logistic",
    "section": "",
    "text": "We‚Äôve had continuous IVs (regression)\nWe‚Äôve had categorical IVs (factorial ANOVA)\nWe‚Äôve had a mixture of continuous/categorical IVs (more regression)\nBut we‚Äôve never had a DV variable that is categorical‚Ä¶ in comes Logistic Regression"
  },
  {
    "objectID": "9-logistic.html#this-whole-semester-has-been-linear-regression-on-steroids",
    "href": "9-logistic.html#this-whole-semester-has-been-linear-regression-on-steroids",
    "title": "Logistic",
    "section": "",
    "text": "We‚Äôve had continuous IVs (regression)\nWe‚Äôve had categorical IVs (factorial ANOVA)\nWe‚Äôve had a mixture of continuous/categorical IVs (more regression)\nBut we‚Äôve never had a DV variable that is categorical‚Ä¶ in comes Logistic Regression"
  },
  {
    "objectID": "9-logistic.html#linear-regression",
    "href": "9-logistic.html#linear-regression",
    "title": "Logistic",
    "section": "Linear Regression",
    "text": "Linear Regression\nWhen we have a continuous DV, we can‚Ä¶ - Calculate the \\(R^2\\) and determine if our IVs & DV are correlated (large value implying = large effect)\n\nCalculate a \\(p\\)-value to determine if \\(R^2\\) (or our model) is statistically significant\nUse the line/slope of a linear regression to make a calculated prediction of y given x\nCompare models\nAdd in predictors & look at interactions"
  },
  {
    "objectID": "9-logistic.html#logistic-regression",
    "href": "9-logistic.html#logistic-regression",
    "title": "Logistic",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nWhat is the goal of prediction when you have a categorial (dichotomous) outcome?\nWe are trying to see if something is TRUE or FALSE"
  },
  {
    "objectID": "9-logistic.html#learning-goals",
    "href": "9-logistic.html#learning-goals",
    "title": "Logistic",
    "section": "Learning Goals",
    "text": "Learning Goals\n\nWTF is logistic regression. When to use it?\nWTF is the generalized linear model. When to use it?\nWTF is maximum likelihood estimation\nHTF do I do this in R"
  },
  {
    "objectID": "9-logistic.html#learning-goals-1",
    "href": "9-logistic.html#learning-goals-1",
    "title": "Logistic",
    "section": "Learning Goals",
    "text": "Learning Goals\nLearning goals today are not: - know every little thing about odds/probabilities/weird things I‚Äôm going to introduce - know every element of the output - memorize anything"
  },
  {
    "objectID": "9-logistic.html#logistic-regression-1",
    "href": "9-logistic.html#logistic-regression-1",
    "title": "Logistic",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nUsed when your DV is binary (0,1)\n- Clinical diagnosis\n- Disease prevalence\n- Experiences (Yes/No) - Correct/Incorrect\n\nThe mean of the distribution is the same as the proportion of 1‚Äôs in the distribution. - Out of 100 people, 27 have PTSD (1) and 73 do not have PTSD (0) - The mean of the distribution is .27 - Probability of getting a 1 is .27\n\nIf we plotted these points, they are either 1‚Äôs or 0‚Äôs\n\n\nIf we plotted our OLS regression line. It doesn‚Äôt make sense! Predicted values can go above 1 or below 0‚Ä¶yikes\n\n\nInstead, we fit this non-linear function. None of our data fall on the line!"
  },
  {
    "objectID": "9-logistic.html#where-is-the-line",
    "href": "9-logistic.html#where-is-the-line",
    "title": "Logistic",
    "section": "Where is the line??",
    "text": "Where is the line??\nLogistic Regression, rather than a straight, fitted line like linear regression, logistic regression fits an S-shaped logistic function.\nThis curved line can tell us the probability that something will be ‚Äú1‚Äù given X"
  },
  {
    "objectID": "9-logistic.html#model-complexity",
    "href": "9-logistic.html#model-complexity",
    "title": "Logistic",
    "section": "Model Complexity",
    "text": "Model Complexity\n‚Ä¶and just like linear regression, we can make simple models (a relationship between x and y, with y being a binary variable) or more complicated models (with covariates).\nUnlike linear regression, it isn‚Äôt as easy to just compare a more complex model to a simple model"
  },
  {
    "objectID": "9-logistic.html#assumption-violations",
    "href": "9-logistic.html#assumption-violations",
    "title": "Logistic",
    "section": "Assumption violations",
    "text": "Assumption violations\nWhen our outcome is binary, we violate OLS regression assumptions\nViolates:\n\nCorrectly specified form (not linear)\nHomoscedasticity (as probability approaches 1 or 0, variance approaches 0)\nNormality of the errors (lol not close)"
  },
  {
    "objectID": "9-logistic.html#need-to-think-in-terms-of-probabilities",
    "href": "9-logistic.html#need-to-think-in-terms-of-probabilities",
    "title": "Logistic",
    "section": "Need to think in terms of probabilities",
    "text": "Need to think in terms of probabilities\n\nIf we use OLS, we violate assumptions and have predicted values that go outside 0 & 1\nHow does the predicted probability of getting a 0 or a 1 relate to our predictors?\n\n\\[\\hat{p}_{i} \\leftrightsquigarrow b_{0} + b_{1}X_{1} + b_{2}X_{2}... b_{3}X_{p}\\]"
  },
  {
    "objectID": "9-logistic.html#generalized-linear-models",
    "href": "9-logistic.html#generalized-linear-models",
    "title": "Logistic",
    "section": "Generalized linear models",
    "text": "Generalized linear models\n\nExtends the general linear model framework\nUsed to describe different Data Generating Processes (DGPs) other than Gaussian normal\nNeed to use if we cannot use the Gaussian normal e.g.¬†the range of Y is restricted (e.g.¬†binary, count) and/or the variance of Y depends on the mean, etc‚Ä¶\nIn other words, what is your DGP?"
  },
  {
    "objectID": "9-logistic.html#generalized-linear-models-1",
    "href": "9-logistic.html#generalized-linear-models-1",
    "title": "Logistic",
    "section": "Generalized linear models",
    "text": "Generalized linear models\n\nGaussian model is \\[y_i \\sim  N(\\mu_i, \\sigma)\\]\n\n\\[\\mu_i = \\alpha + \\beta x_i\\] - The parameters of the normal distribution are the mean Œº and the standard deviation œÉ (or the variance \\(\\sigma^2\\))]\n\nBinomial outcome is \\[y_i \\sim  Binomial(n, p_i)\\]\n\n\\[f(p_i) = \\alpha + \\beta x_i\\] - n stands for the number of times the experiment runs. - p represents the probability of one specific outcome.]"
  },
  {
    "objectID": "9-logistic.html#glm-components",
    "href": "9-logistic.html#glm-components",
    "title": "Logistic",
    "section": "GLM components",
    "text": "GLM components\nIt is not common to have an average outcome \\(\\mu\\) (other than in a normal distribution)\nIt is not common to have parameters range from negative to positive infinity.\nWe need something that translates our model into the parameters that describe the distribution."
  },
  {
    "objectID": "9-logistic.html#logistic-regression-2",
    "href": "9-logistic.html#logistic-regression-2",
    "title": "Logistic",
    "section": "Logistic regression",
    "text": "Logistic regression\n\\[y_i \\sim  Binomial(n, p_i)\\]\n\\[f(p_i) = \\alpha + \\beta x_i\\]\nOur number of trials is \\(N\\), so we are predicting the probability of \\(y\\). Probabilities are bounded between zero and one.\nBecause our variables are not in probability units we need to ‚Äúlink‚Äù them via a function. The linear right side of the equation is not in the same units as the non-linear left side.\nTwo most popular are logit and log. Others are available too, such as probit."
  },
  {
    "objectID": "9-logistic.html#link-function-for-logistic",
    "href": "9-logistic.html#link-function-for-logistic",
    "title": "Logistic",
    "section": "Link function for logistic",
    "text": "Link function for logistic\n\nWe need to map (0,1) to \\((-\\infty, \\infty)\\)\nLogistic regression uses the logistic function to link the predicted probabilities to the predictors\nThink of it as a transformation of \\(\\hat{Y}\\)s"
  },
  {
    "objectID": "9-logistic.html#sigmoid-function",
    "href": "9-logistic.html#sigmoid-function",
    "title": "Logistic",
    "section": "Sigmoid function",
    "text": "Sigmoid function\n\\[f(x) = \\frac{1}{1+e^{-X}}\\]\n‚Ä¶ where \\(e\\) is Euler‚Äôs number\n\nexp(1)\n\n[1] 2.718282\n\n\n\nThe y-axis is the ‚Äúrolling mean‚Äù of the DV (or the proportion of 1‚Äôs). This logistic curve relates \\(X\\) (the IV) to our \\(P(\\bar{Y})\\)"
  },
  {
    "objectID": "9-logistic.html#sigmoid-function-cont.",
    "href": "9-logistic.html#sigmoid-function-cont.",
    "title": "Logistic",
    "section": "Sigmoid function (cont.)",
    "text": "Sigmoid function (cont.)\nFor the Sigmoid function, as x approaches \\(\\infty\\), it reaches a natural limit at 1.\nAs x approaches \\(-\\infty\\), it reaches a natural limit of 0\nThis all keeps the limits within the 0, 1 range.\n\n\\[f(x) = \\frac{1}{1+e^{-X}}\\] \\[\\hat{p} = \\frac{1}{1+e^{-b_{0}+b_{1}X}}\\] - The form of the logistic function is still nonlinear (because probabilities can only range from 0 to 1)\n\nSince it‚Äôs nonlinear, \\(b\\) can‚Äôt be interpreted as easily as we‚Äôve been doing with OLS regression.\nSo in order to interpret our model parameter, we need to convert to odds\n\n\nwhere P is the probability of a 1 (the proportion of 1s, the mean of Y), e is the base of the natural logarithm (about 2.718) and a and b are the parameters of the model. The value of a yields P when X is zero, and b adjusts how quickly the probability changes with changing X a single unit (we can have standardized and unstandardized b weights in logistic regression, just as in ordinary linear regression). Because the relation between X and P is nonlinear, b does not have a straightforward interpretation in this model as it does in ordinary linear regression"
  },
  {
    "objectID": "9-logistic.html#odds",
    "href": "9-logistic.html#odds",
    "title": "Logistic",
    "section": "Odds",
    "text": "Odds\n\nOdds are defined as the probability of being a case divided by the probability of being a noncase\nNot bound between 0 and 1\nRange from 0 to infinity\nLess than one is less than 50% probability \\[odds = \\frac {\\hat{p}}{1-\\hat{p}}\\]\n\n\\[probability= \\frac{\\hat{odds}}{1+\\hat{odds}}\\]"
  },
  {
    "objectID": "9-logistic.html#odds-versus-probability",
    "href": "9-logistic.html#odds-versus-probability",
    "title": "Logistic",
    "section": "Odds versus Probability",
    "text": "Odds versus Probability\nPut simply:\n\nOdds are the ratio of something happening over something not happening.\nProbability is the ratio of something happening over everything that could happen.\n\nThe odds being so asymmetrical (unlike probability) make it difficult to compare the odds of being 0 versus the odds of being 1. So we use something called log odds."
  },
  {
    "objectID": "9-logistic.html#log-odds",
    "href": "9-logistic.html#log-odds",
    "title": "Logistic",
    "section": "Log Odds",
    "text": "Log Odds\nTaking the log of the odds solves this problem by making everything symetrical!\nOdds are 1 to 6:\n\nlog(1/6) = log(0.17) = -1.79\n\nOdds are 6 to 1:\n\nlog(6/1) = log(6) = 1.79"
  },
  {
    "objectID": "9-logistic.html#linear-probability-model",
    "href": "9-logistic.html#linear-probability-model",
    "title": "Logistic",
    "section": "Linear Probability Model",
    "text": "Linear Probability Model\n\\[f(x) = \\frac{1}{1+e^{-X}}\\] \\[\\hat{p} = \\frac{1}{1+e^{-b_{0}+b_{1}X}}\\]\n\\[odds = \\frac {\\hat{p}}{1-\\hat{p}}=e^{b_{0}+b_{1}X}\\] \\[logit= Log(odds)=ln(\\frac{\\hat{p}}{1-\\hat{p}}) = b_{0}+b_{1}X\\]"
  },
  {
    "objectID": "9-logistic.html#logit",
    "href": "9-logistic.html#logit",
    "title": "Logistic",
    "section": "Logit",
    "text": "Logit\n\\[logit= Log(odds)=ln(\\frac{\\hat{p}}{1-\\hat{p}}) = b_{0}+b_{1}X\\] - DV is a logit, the natural log of odds\n\nPredicted scores are not dichotomous\nInstead of predicting probabilities directly, we are instead predicting the log of the odds.\nThe regression we are used to is not predicting \\(\\hat{Y}\\), because we‚Äôre predicting (or relating our IV) logits"
  },
  {
    "objectID": "9-logistic.html#now-what",
    "href": "9-logistic.html#now-what",
    "title": "Logistic",
    "section": "Now what?",
    "text": "Now what?\n\nIn order to get the probability of a value of X being 1 or 0 based on some model parameters ( \\(b_0\\), \\(b_1\\) etc. ), we rearrange our equation so that we actually get the predicted logit (log of odds) given our model parameters.\nHow the hell do you interpret a logit? Not easily‚Ä¶\nConvert back into odds\nConvert your odds back into probabilities"
  },
  {
    "objectID": "9-logistic.html#uhhhhhh",
    "href": "9-logistic.html#uhhhhhh",
    "title": "Logistic",
    "section": "Uhhhhhh",
    "text": "Uhhhhhh\nDoes this seem convoluted?\nWe are predicting logits, which happen to be the log of odds. It‚Äôs bananas.\nHow do we get these numbers in the first place? Can we use OLS?"
  },
  {
    "objectID": "9-logistic.html#estimation-with-maximum-likelihood",
    "href": "9-logistic.html#estimation-with-maximum-likelihood",
    "title": "Logistic",
    "section": "Estimation with Maximum Likelihood",
    "text": "Estimation with Maximum Likelihood\n\nOLS minimizes the errors ( \\(SS_{res}\\)), which maximizes ( \\(SS_{reg}\\))\n\nIn logistic regression we are not so lucky\nNeed to rely on iterative procedure, Maximum Likelihood (ML) Estimation:\n\nPick parameters of your model ( \\(b_0\\), \\(b_1\\) etc. ), and calculate the likelihood of the data, given those parameters. We do this iteratively until we find the best parameters ‚Äì the ones that maximize the likelihood of your data."
  },
  {
    "objectID": "9-logistic.html#estimation-with-maximium-likelihood",
    "href": "9-logistic.html#estimation-with-maximium-likelihood",
    "title": "Logistic",
    "section": "Estimation with Maximium Likelihood",
    "text": "Estimation with Maximium Likelihood\nDo any of you hike? Know how to read a topographic map?\nML Estimation is sort of like going hiking with your data to find the highest point. And most of the time you can!\nBut if if you take a wrong turn, you might get screwed. It‚Äôs the risk we take when using ML Estimation!"
  },
  {
    "objectID": "9-logistic.html#estimation-with-maximum-likelihood-1",
    "href": "9-logistic.html#estimation-with-maximum-likelihood-1",
    "title": "Logistic",
    "section": "Estimation with Maximum Likelihood",
    "text": "Estimation with Maximum Likelihood\nWe are using the distribution of the data to find the location that maximizes the likelihood of observing the variable that we measured.\nWe are essentially trying to find the optimal value for the mean (or standard deviation) for a distribution given our observed data.\nWe are talking here about the mean of the distribution, not the mean of the data. (However, in a normal distribution, these are the same thing.)"
  },
  {
    "objectID": "9-logistic.html#estimation-with-maximum-likelihood-2",
    "href": "9-logistic.html#estimation-with-maximum-likelihood-2",
    "title": "Logistic",
    "section": "Estimation with Maximum Likelihood",
    "text": "Estimation with Maximum Likelihood\n\nSo we get model parameters, estimated via MLE (instead of OLS)\nSame but different:\n\nAsymptotic standard errors (an approximation to the standard error)\nInterpret test statistics as \\(z\\)‚Äôs, not \\(t\\)‚Äôs\nNo \\(t\\)-tests; instead a Wald test = \\(\\chi^2\\) test with 1 df = \\((\\frac{coef}{se})^2\\)\n\n\nasymptotic se‚Äôs are just approximations since we finite-sample dist of the estimator isn‚Äôt known"
  },
  {
    "objectID": "9-logistic.html#all-together-now",
    "href": "9-logistic.html#all-together-now",
    "title": "Logistic",
    "section": "All Together Now",
    "text": "All Together Now\n\nWe try to predict binary outcomes. Our DV is dichotomous.\nWe can use categorical or continuous predictors (IVs) to do so\nWe can write the equation in roughly the same way\nWe cannot use OLS because it will give us impossible values and just logically it doesn‚Äôt work. We use ML Estimation to determine our logistic regression coefficients instead. This gives us a S-shaped curve\nWe can ask if our coefficients are significantly different from 0\nWe convert to odds and/or probabilities"
  },
  {
    "objectID": "9-logistic.html#glm-in-r",
    "href": "9-logistic.html#glm-in-r",
    "title": "Logistic",
    "section": "GLM in R",
    "text": "GLM in R\n\nglm(formula,\n    family = gaussian(link=\"identity\"), #&lt;&lt;\n    data,\n    weights,\n    subset,\n    na.action,\n    start = NULL,\n    etastart,\n    mustart,\n    offset, \n    control = glm.control(...),\n    model = TRUE,\n    method = ‚Äùglm.fit‚Äù,\n    x = FALSE, \n    y = TRUE,\n    contrasts = NULL, ...)\n\n\nThe family argument specifies the distribution. In R, families have default links."
  },
  {
    "objectID": "9-logistic.html#glm-in-r-1",
    "href": "9-logistic.html#glm-in-r-1",
    "title": "Logistic",
    "section": "GLM in R",
    "text": "GLM in R\n\nglm(y ~ X1+ X2 + X3 , #&lt;&lt;\n    family = binomial,\n    data = dataset)\n\nSpecify the model like you would with lm"
  },
  {
    "objectID": "9-logistic.html#glm-in-r-2",
    "href": "9-logistic.html#glm-in-r-2",
    "title": "Logistic",
    "section": "GLM in R",
    "text": "GLM in R\n\nglm(y ~ X1+ X2 + X3 , \n    family = binomial, #&lt;&lt;\n    data = dataset)\n\nSpecify the distribution you‚Äôre working with. When binary outcomes, we‚Äôll use the binomial."
  },
  {
    "objectID": "9-logistic.html#glm-in-r-3",
    "href": "9-logistic.html#glm-in-r-3",
    "title": "Logistic",
    "section": "GLM in R",
    "text": "GLM in R\n\nglm(y ~ X1+ X2 + X3 , \n    family = binomial, \n    data = dataset) #&lt;&lt;\n\nSpecify your dataset."
  },
  {
    "objectID": "9-logistic.html#how-to-interpret",
    "href": "9-logistic.html#how-to-interpret",
    "title": "Logistic",
    "section": "How to interpret",
    "text": "How to interpret\n\n\\(b_1\\) is the predicted change in the logit for a 1-unit change in X, holding the other predictors constant\nFor a 1-unit change in X, holding other predictors constant, the odds that Y = 1 changes by \\(e^{b_{1}}\\)\n\ne.g,. \\(b_{1}\\) = .4, \\(e^{.4}\\) = 1.49\n\nFor fitted values, need to use entire equation \\(\\hat{Y} = e^{b_{0}+b_{1}X_{1}}\\)\nTurn to probabilities by: \\(\\frac{\\text{odds}}{(1 + \\text{odds})}\\)"
  },
  {
    "objectID": "9-logistic.html#example",
    "href": "9-logistic.html#example",
    "title": "Logistic",
    "section": "Example",
    "text": "Example\n\n# 1 = not premature\nmortality\n\n# A tibble: 300 √ó 4\n   Intelligence_Self Intelligence_Mate premature.d NOT.premature\n               &lt;dbl&gt;             &lt;dbl&gt; &lt;fct&gt;               &lt;dbl&gt;\n 1                22                19 normal                  1\n 2                22                18 normal                  1\n 3                21                21 normal                  1\n 4                22                17 normal                  1\n 5                19                18 normal                  1\n 6                19                20 premature               0\n 7                16                18 normal                  1\n 8                15                11 premature               0\n 9                16                21 normal                  1\n10                19                22 normal                  1\n# ‚Ñπ 290 more rows\n\n\n\n\ndeath.1 &lt;- lm(NOT.premature ~ Intelligence_Self , data = mortality)\nsummary(death.1)\n\n\nCall:\nlm(formula = NOT.premature ~ Intelligence_Self, data = mortality)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.9030  0.1084  0.1538  0.1907  0.3355 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       0.641769   0.098636   6.506 3.25e-10 ***\nIntelligence_Self 0.011357   0.005807   1.956   0.0514 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3745 on 298 degrees of freedom\nMultiple R-squared:  0.01267,   Adjusted R-squared:  0.009362 \nF-statistic: 3.826 on 1 and 298 DF,  p-value: 0.05141\n\n\n\n\ndeath.2 &lt;- glm(NOT.premature ~ Intelligence_Self , data = mortality)\nsummary(death.2)\n\n\nCall:\nglm(formula = NOT.premature ~ Intelligence_Self, data = mortality)\n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       0.641769   0.098636   6.506 3.25e-10 ***\nIntelligence_Self 0.011357   0.005807   1.956   0.0514 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.1402466)\n\n    Null deviance: 42.330  on 299  degrees of freedom\nResidual deviance: 41.793  on 298  degrees of freedom\nAIC: 266.05\n\nNumber of Fisher Scoring iterations: 2\n\n\n\n\ndeath.3 &lt;- glm(NOT.premature ~ Intelligence_Self,\n               family = binomial, data = mortality)\nsummary(death.3)\n\n\nCall:\nglm(formula = NOT.premature ~ Intelligence_Self, family = binomial, \n    data = mortality)\n\nCoefficients:\n                  Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)        0.28695    0.67490   0.425   0.6707  \nIntelligence_Self  0.08012    0.04143   1.934   0.0532 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 273.53  on 299  degrees of freedom\nResidual deviance: 269.75  on 298  degrees of freedom\nAIC: 273.75\n\nNumber of Fisher Scoring iterations: 4"
  },
  {
    "objectID": "9-logistic.html#interpretation",
    "href": "9-logistic.html#interpretation",
    "title": "Logistic",
    "section": "Interpretation",
    "text": "Interpretation\nFor a 1-unit change in X, holding other predictors constant, the odds that Y = 1 changes by \\(e^{b_{1}}\\)\n\nexp(1)^.08012\n\n[1] 1.083417\n\n\nFor every 1-unit increase in Intelligence, the odds of not having a premature death 8%"
  },
  {
    "objectID": "9-logistic.html#specific-values",
    "href": "9-logistic.html#specific-values",
    "title": "Logistic",
    "section": "Specific Values?",
    "text": "Specific Values?\nWhat if you want the probability of being a premature death for a given level of Intelligence? (Now that we‚Äôve run our model and have parameters‚Ä¶)\nFor fitted values, need to use entire equation \\(\\hat{Y} = e^{b_{0}+b_{1}X_{1}}\\)\n\n# get fitted value with a given value of X (here 20)\nexp(1)^(0.28695 + (.08012*20))\n\n[1] 6.615067\n\n# now get odds\n6.615067 / (1+6.615067)\n\n[1] 0.8686814"
  },
  {
    "objectID": "9-logistic.html#probit",
    "href": "9-logistic.html#probit",
    "title": "Logistic",
    "section": "Probit",
    "text": "Probit\nWe can have different link functions. When your response variable (DV) is truly binary ‚Äì the data generating process generates legit binary data ‚Äì logit is your pick.\n\nWhat if your response variable is binary, but the underlying construct you are trying to measure is likely Gaussian? Ex: depressed vs.¬†not depressed. But the underlying latent construct is continuous. More appropriate then is the probit link function.\nStack exchange thread if you‚Äôre going down this route"
  },
  {
    "objectID": "9-logistic.html#probit-1",
    "href": "9-logistic.html#probit-1",
    "title": "Logistic",
    "section": "Probit",
    "text": "Probit\n\ndeath.4 &lt;- glm(NOT.premature ~ Intelligence_Self,\n        family = binomial(link = \"probit\"), data = mortality)\nsummary(death.4)\n\n\nCall:\nglm(formula = NOT.premature ~ Intelligence_Self, family = binomial(link = \"probit\"), \n    data = mortality)\n\nCoefficients:\n                  Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)        0.21961    0.38376   0.572   0.5671  \nIntelligence_Self  0.04513    0.02319   1.946   0.0516 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 273.53  on 299  degrees of freedom\nResidual deviance: 269.72  on 298  degrees of freedom\nAIC: 273.72\n\nNumber of Fisher Scoring iterations: 4"
  },
  {
    "objectID": "10-bayes.html",
    "href": "10-bayes.html",
    "title": "Bayes",
    "section": "",
    "text": "Not drastically different!\nYou get to keep everything you like\nYour models stay the same!!"
  },
  {
    "objectID": "10-bayes.html#reassurance-before-we-get-to-the-three-steps",
    "href": "10-bayes.html#reassurance-before-we-get-to-the-three-steps",
    "title": "Bayes",
    "section": "",
    "text": "Not drastically different!\nYou get to keep everything you like\nYour models stay the same!!"
  },
  {
    "objectID": "10-bayes.html#glm",
    "href": "10-bayes.html#glm",
    "title": "Bayes",
    "section": "GLM",
    "text": "GLM\n\nOur good friend that gives us 99% of the models psychologists use (general(ized) linear model), is exactly the same\n\n\\[\\Large Y = b_{0} + b_{1}X +e\\] - No need to think about setting up new t-test, ANOVAS, regressions, etc. ALL THE SAME."
  },
  {
    "objectID": "10-bayes.html#a-working-mental-model",
    "href": "10-bayes.html#a-working-mental-model",
    "title": "Bayes",
    "section": "A working mental model",
    "text": "A working mental model\nWhat are Bayesian models?\n\n‚ÄúNormal‚Äù regression with a different algorithm.\nResults that represent a distribution rather than a point estimate and some uncertainty.\nPriors that incorporate existing knowledge."
  },
  {
    "objectID": "10-bayes.html#be-comfortable-with-a-different-estimation-algorithm",
    "href": "10-bayes.html#be-comfortable-with-a-different-estimation-algorithm",
    "title": "Bayes",
    "section": "1. Be comfortable with a different estimation algorithm",
    "text": "1. Be comfortable with a different estimation algorithm\n\nWhat do you mean by estimation algorithm?\n\n\n\nOLS i.e.¬†\\(min\\sum(Y_{i}-\\hat{Y})^{2}\\)\nFun fact, R uses QR decomposition, Newton Raphson, Fisher Scoring, SVR, etc ‚Äì not this equation.\nAnother fun fact, more advanced stats use an even different algorithm (e.g., maximum likelihood)"
  },
  {
    "objectID": "10-bayes.html#standard-way",
    "href": "10-bayes.html#standard-way",
    "title": "Bayes",
    "section": "Standard way",
    "text": "Standard way\n\n\nCode\nlibrary(tidyverse)\n\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.5\n‚úî forcats   1.0.0     ‚úî stringr   1.5.1\n‚úî ggplot2   3.5.1     ‚úî tibble    3.2.1\n‚úî lubridate 1.9.4     ‚úî tidyr     1.3.1\n‚úî purrr     1.0.2     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\ngalton.data &lt;- psychTools::galton\n\n\n\n\n\n\n\n\n\n\nCode\nfit.1 &lt;- lm(child ~ parent, data = galton.data)\nsummary(fit.1)\n\n\n\nCall:\nlm(formula = child ~ parent, data = galton.data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.8050 -1.3661  0.0487  1.6339  5.9264 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 23.94153    2.81088   8.517   &lt;2e-16 ***\nparent       0.64629    0.04114  15.711   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.239 on 926 degrees of freedom\nMultiple R-squared:  0.2105,    Adjusted R-squared:  0.2096 \nF-statistic: 246.8 on 1 and 926 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "10-bayes.html#fisher-scoring",
    "href": "10-bayes.html#fisher-scoring",
    "title": "Bayes",
    "section": "Fisher Scoring",
    "text": "Fisher Scoring\n\n\nCode\nfit.1.g &lt;- glm(child ~ parent, family = gaussian, data = galton.data)\nsummary(fit.1.g)\n\n\n\nCall:\nglm(formula = child ~ parent, family = gaussian, data = galton.data)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 23.94153    2.81088   8.517   &lt;2e-16 ***\nparent       0.64629    0.04114  15.711   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 5.011094)\n\n    Null deviance: 5877.2  on 927  degrees of freedom\nResidual deviance: 4640.3  on 926  degrees of freedom\nAIC: 4133.2\n\nNumber of Fisher Scoring iterations: 2"
  },
  {
    "objectID": "10-bayes.html#maximum-likelihood",
    "href": "10-bayes.html#maximum-likelihood",
    "title": "Bayes",
    "section": "Maximum likelihood",
    "text": "Maximum likelihood\n\n\nThis is lavaan 0.6-19\nlavaan is FREE software! Please report any bugs.\n\n\nlavaan 0.6-19 ended normally after 1 iteration\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         3\n\n  Number of observations                           928\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.000\n  Degrees of freedom                                 0\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  child ~                                             \n    parent            0.646    0.041   15.728    0.000\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .child            23.942    2.808    8.527    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .child             5.000    0.232   21.541    0.000"
  },
  {
    "objectID": "10-bayes.html#bayesian-way",
    "href": "10-bayes.html#bayesian-way",
    "title": "Bayes",
    "section": "Bayesian way",
    "text": "Bayesian way\n\n\nCode\nlibrary(brms)\n\n\nLoading required package: Rcpp\n\n\nLoading 'brms' package (version 2.22.0). Useful instructions\ncan be found by typing help('brms'). A more detailed introduction\nto the package is available through vignette('brms_overview').\n\n\n\nAttaching package: 'brms'\n\n\nThe following object is masked from 'package:stats':\n\n    ar\n\n\nCode\nfit.1.bayesian &lt;- brm(child ~ parent, data = galton.data, backend = \"cmdstanr\", file = \"fit.1.b\")\n\n\n\nsummary(fit.1.bayesian)\n\nLoading required package: rstan\n\n\nLoading required package: StanHeaders\n\n\n\nrstan version 2.36.0.9000 (Stan version 2.35.0)\n\n\nFor execution on a local, multicore CPU with excess RAM we recommend calling\noptions(mc.cores = parallel::detectCores()).\nTo avoid recompilation of unchanged Stan programs, we recommend calling\nrstan_options(auto_write = TRUE)\nFor within-chain threading using `reduce_sum()` or `map_rect()` Stan functions,\nchange `threads_per_chain` option:\nrstan_options(threads_per_chain = 1)\n\n\n\nAttaching package: 'rstan'\n\n\nThe following object is masked from 'package:tidyr':\n\n    extract\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: child ~ parent \n   Data: galton.data (Number of observations: 928) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    24.00      2.79    18.54    29.38 1.00     3554     3112\nparent        0.65      0.04     0.57     0.73 1.00     3545     3112\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     2.24      0.05     2.14     2.34 1.00     3880     3058\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\n\n\nCode\nplot(conditional_effects(fit.1.bayesian), points = TRUE)"
  },
  {
    "objectID": "10-bayes.html#step-1-is-easy",
    "href": "10-bayes.html#step-1-is-easy",
    "title": "Bayes",
    "section": "Step 1 is easy",
    "text": "Step 1 is easy\n\nBayes gives you basically the same results\n\n\n\nSo why use it? Many reasons, but the most direct is manipulating, visualizing, and extrapolating from results"
  },
  {
    "objectID": "10-bayes.html#think-of-results-in-terms-of-distributions",
    "href": "10-bayes.html#think-of-results-in-terms-of-distributions",
    "title": "Bayes",
    "section": "2. Think of results in terms of distributions",
    "text": "2. Think of results in terms of distributions\n\nWhat are results?\n\n\n\nEstimate and an SE\nIndicates a ‚Äúbest guess‚Äù ie mean/median/mode and the imprecision related to it\nIf this guess is far away from zero (and imprecision not large), then it is significant\nWe know that if we repeated this again we won‚Äôt get the same answer (estimate), but likely in between our CIs\nHow do we convey the ‚Äúbest guess?‚Äù\n\n\n\n\n\nCode\nlibrary(tidybayes)\n\n\n\nAttaching package: 'tidybayes'\n\n\nThe following objects are masked from 'package:brms':\n\n    dstudent_t, pstudent_t, qstudent_t, rstudent_t\n\n\nCode\nfit.1.bayesian %&gt;% \nspread_draws(b_Intercept, b_parent) %&gt;% \nselect(b_parent) %&gt;% \n  mode_hdi(.width = c(.95)) %&gt;%  \nggplot(aes(y = as.factor(.width), x = b_parent, xmin = .lower, xmax = .upper)) + geom_pointinterval() + ylab(\"\")\n\n\n\n\n\n\n\nThe problem is they obscure information\n\n\n\nCode\nfit.1.bayesian %&gt;% \nspread_draws(b_Intercept, b_parent) %&gt;% \nggplot(aes(x = b_parent)) +\n  stat_halfeye()"
  },
  {
    "objectID": "10-bayes.html#posterior-distribution-ie-results",
    "href": "10-bayes.html#posterior-distribution-ie-results",
    "title": "Bayes",
    "section": "Posterior distribution (ie results)",
    "text": "Posterior distribution (ie results)\n\nIs made up of a series of educated guesses (via our algorithm), each of which is consistent with the data.\nIn aggregate, these guesses provide us not with a best guess and an SD (as with Maximum Likelihood), but a more complete sense of each parameter we are trying to estimate.\nWe can assume this distribution (typically normal) with standard estimation, but with bayes it can be flexible!"
  },
  {
    "objectID": "10-bayes.html#posterior-distribution-ie-results-1",
    "href": "10-bayes.html#posterior-distribution-ie-results-1",
    "title": "Bayes",
    "section": "Posterior distribution (ie results)",
    "text": "Posterior distribution (ie results)\nIs made of up of a series of educated guesses. Each dot represents a particular guess. Guesses that occur more often are considered more likely.\n\n\nCode\nfit.1.bayesian %&gt;% \nspread_draws(b_Intercept, b_parent) %&gt;% \nggplot(aes(x = b_parent)) +\n  stat_dotsinterval()"
  },
  {
    "objectID": "10-bayes.html#how-does-the-algorithm-work",
    "href": "10-bayes.html#how-does-the-algorithm-work",
    "title": "Bayes",
    "section": "How does the algorithm work?",
    "text": "How does the algorithm work?\n\nPlayed a role in developing the thermonuclear bomb with one of the earliest computers. Published in 1953 but ignored within stats b/c it was published within a physics/chemistry journal. Took about until 1990 for desktop computers to run fast enough to do at home.\nMany variants, but the general idea is a) propose an estimate value + noise N(0, \\(\\sigma\\) ) then b) see how ‚Äúlikely‚Äù the data is given the estimate, c) based on some criteria (better than worse that some value) either accept or reject the estimate and d) repeat"
  },
  {
    "objectID": "10-bayes.html#what-do-you-mean-by-likely",
    "href": "10-bayes.html#what-do-you-mean-by-likely",
    "title": "Bayes",
    "section": "What do you mean by likely?",
    "text": "What do you mean by likely?\nYou‚Äôve done this before last semester. Three parameters in a binomial distribution (# successes, # of trials, probability of success). Often you would fix #trials and probability of success to see what # successes are most/least likely.\n\n\nCode\ndata.frame(heads = 0:10, prob = dbinom(x = 0:10, size = 10, prob = .5)) %&gt;% \n  ggplot(aes(x = factor(heads), y = prob)) +\n  geom_col(fill = \"#562457\") +\n  geom_text(aes(label = round(prob, 2), y = prob + .01),\n            position = position_dodge(.9),\n            size = 5, \n            vjust = 0) +\n  labs(title = \"Binomial Distribution of Coin Flips\",\n       subtitle = \"n = 10, p = .5\",\n       x = \"Number of Successes (Heads)\",\n       y = \"Density\") +\n  theme_classic(base_size = 16)\n\n\n\n\n\n\n\nBut we often don‚Äôt know what P is. That is the parameter we want to estimate. But we collected data! So we can look at what p is consistent (or not) with our data (2 successes in 10 trials).\nThis is basically what our current ML algorithms do.\n\n\n\nCode\np &lt;- seq(0.01, 0.99, by = 0.01)\nloglike &lt;- dbinom(2, size = 10, prob = p)\nplot(loglike)\n\n\n\n\n\n\n\nThe Bayesian (MCMC) algorithm tries out a bunch of parameter values. The one‚Äôs that are more likely will appear more often.\nWhat do I mean ‚Äúappear‚Äù more often. The algorithm lands on that just as our coin flipping example finds .2 to be most likely.\n\n\n\nCode\nfit.1.bayesian %&gt;% \nspread_draws(b_Intercept, b_parent) %&gt;% \nggplot(aes(x = b_parent)) +\n  stat_dotsinterval()\n\n\n\n\n\n\nOur posterior is literally made up of educated guesses by the algorithm\n\ntidy_draws(fit.1.bayesian)\n\n# A tibble: 4,000 √ó 14\n   .chain .iteration .draw b_Intercept b_parent sigma lprior   lp__\n    &lt;int&gt;      &lt;int&gt; &lt;int&gt;       &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1      1          1     1        21.4    0.684  2.18  -3.83 -2068.\n 2      1          2     2        22.5    0.667  2.13  -3.82 -2069.\n 3      1          3     3        18.2    0.730  2.22  -3.84 -2069.\n 4      1          4     4        27.0    0.602  2.24  -3.85 -2068.\n 5      1          5     5        20.3    0.699  2.24  -3.85 -2068.\n 6      1          6     6        27.7    0.592  2.23  -3.84 -2068.\n 7      1          7     7        25.6    0.622  2.22  -3.84 -2067.\n 8      1          8     8        25.9    0.618  2.28  -3.86 -2067.\n 9      1          9     9        24.4    0.638  2.35  -3.88 -2070.\n10      1         10    10        23.9    0.648  2.12  -3.82 -2070.\n# ‚Ñπ 3,990 more rows\n# ‚Ñπ 6 more variables: accept_stat__ &lt;dbl&gt;, treedepth__ &lt;dbl&gt;, stepsize__ &lt;dbl&gt;,\n#   divergent__ &lt;dbl&gt;, n_leapfrog__ &lt;dbl&gt;, energy__ &lt;dbl&gt;"
  },
  {
    "objectID": "10-bayes.html#more-intuituion",
    "href": "10-bayes.html#more-intuituion",
    "title": "Bayes",
    "section": "More intuituion",
    "text": "More intuituion\n\nThink of the algorithm as picking out marbles from a sack, with replacement, to figure out the distribution of colors.\nOr us doing rnorm with me hiding what the mean and SD are, but then figuring out what the mean and SD are through counting the samples.\n\n\n\n\nCode\nsack &lt;- as_tibble(rnorm(1000000, mean = 100, sd = 15))\nggplot(sack, aes(x=value)) + \n  geom_density()"
  },
  {
    "objectID": "10-bayes.html#bayesian-analysis-is-just-counting",
    "href": "10-bayes.html#bayesian-analysis-is-just-counting",
    "title": "Bayes",
    "section": "Bayesian analysis is just counting",
    "text": "Bayesian analysis is just counting\n\nBayesian analysis counts all ways that something can happen (according to assumptions/model). Assumptions with more ways that are consistent with data are more plausible.\nThis method is not demonstrably different than standard approaches. Standard likelihood approaches use the values that are most consistent with the data as an estimate. Try out all possible numbers and then tells you which one is most likely.\nWhere Bayes differs, is we will focus beyond just a ‚Äúbest estimate‚Äù"
  },
  {
    "objectID": "10-bayes.html#visualizing-uncertainty",
    "href": "10-bayes.html#visualizing-uncertainty",
    "title": "Bayes",
    "section": "Visualizing uncertainty",
    "text": "Visualizing uncertainty\nOur posterior (ie different educated guesses at a the correct parameters; distribution of plausible values) is highlighting: that there is no ONE result, that there are many possible results that are consistent with the data.\n\n\nCode\nfit.1.bayesian %&gt;% \nspread_draws(b_Intercept, b_parent) %&gt;% \nggplot(aes(x = b_parent)) +\n  stat_dotsinterval()"
  },
  {
    "objectID": "10-bayes.html#some-positives-of-focusing-on-uncertainty",
    "href": "10-bayes.html#some-positives-of-focusing-on-uncertainty",
    "title": "Bayes",
    "section": "Some positives of focusing on uncertainty",
    "text": "Some positives of focusing on uncertainty\n\nDo not need to assume normal or multivariate normal. Uncertainty does not need to be even tailed.\nDifferences (say across groups) in uncertainty is allowed. Do not need to assume groups have same standard errors. One can better account for and/or probe situations where a certain group has a lot or little variability.\nEasy to calculate uncertainty"
  },
  {
    "objectID": "10-bayes.html#cis-around-a-particular-value",
    "href": "10-bayes.html#cis-around-a-particular-value",
    "title": "Bayes",
    "section": "CIs around a particular value",
    "text": "CIs around a particular value\nWith your current knowledge, calculate a 95% CI around parent = 72 inches, to tell you what is possible for the sample mean at that hight.\n\\[  \\hat{Y}\\pm t_{critical} * se_{residual}*\\sqrt{\\frac {1}{n}+\\frac{(X-\\bar{X})^2}{(n-1)s_{X}^2}}  \\]\n\n\n\nCode\nfit.1.bayesian %&gt;% \nspread_draws(b_Intercept, b_parent) %&gt;% \n  select(b_Intercept,b_parent) %&gt;% \n  mutate(mu_at_72 = b_Intercept + (b_parent * 72)) %&gt;%  ggplot(aes(x = mu_at_72)) +\n  stat_halfeye() +\n  xlab(expression(mu[\"C-height at parent 72\"]))"
  },
  {
    "objectID": "10-bayes.html#be-comfortable-integrating-prior-knowledge",
    "href": "10-bayes.html#be-comfortable-integrating-prior-knowledge",
    "title": "Bayes",
    "section": "3. Be comfortable integrating prior knowledge",
    "text": "3. Be comfortable integrating prior knowledge\n\nPriors insert knowledge you have outside of your data into your model\nThis can seem ‚Äúsubjective‚Äù as opposed to the more ‚Äúobjective‚Äù way of letting the data speak.\n\n\n\nWe will mostly not ‚Äútip the scales‚Äù towards an outcome we want.\nMost of the time the prior knowledge constrains plausible or implausible range of values e.g.¬†we know an effect size of a million is very unlikely.\nOften priors don‚Äôt matter‚Ä¶\n\n\n\n\nTake our height example where we are fitting \\(\\Large Child = b_{0} + b_{1}Parent +e\\)\nWe need to put priors on each parameter we want to estimate, here \\(b_{0}\\) & \\(b_{1}\\) (and e).\n\\(b_{0}\\) is the intercept and reflect average child height when parent height is centered.\nWe know, roughly, what average height of adults are so we can create a distribution, say ~N(66 (5.5 ft), 5). That means we are pretty sure (95%) the average height is between ~4‚Äô8 and 6`4\n\n\n\n\nCode\np.0 &lt;-\n  tibble(x = seq(from = 40, to = 100, by = .1)) %&gt;% \n  \n  ggplot(aes(x = x, y = dnorm(x, mean = 66, sd = 5))) +\n  geom_line() +\n  scale_x_continuous(breaks = seq(from = 40, to = 100, by = 10)) +\n  labs(title = \"mu ~ dnorm(66, 5)\",\n       y = \"density\")\n\np.0\n\n\n\n\n\n\n\nWe could argue that the \\(b_{1}\\) parameter (which indexes the strength of association between parent and child height) is positive. But we don‚Äôt want to stack the deck.\nLet‚Äôs center it around zero, saying that the most plausible estimate is no association, but that we are willing to entertain some strong effects in either direction.\n\n\n\n\nCode\np.1 &lt;-\n  tibble(x = seq(from = -15, to = 15, by = .1)) %&gt;% \n  \n  ggplot(aes(x = x, y = dnorm(x, mean = 0, sd = 5))) +\n  geom_line() +\n  scale_x_continuous(breaks = seq(from = -15, to = 15, by = 3)) +\n  labs(title = \"mu ~ dnorm(0, 5)\",\n       y = \"density\")\n\np.1"
  },
  {
    "objectID": "10-bayes.html#okay-so-what-does-this-mean",
    "href": "10-bayes.html#okay-so-what-does-this-mean",
    "title": "Bayes",
    "section": "Okay so what does this mean?",
    "text": "Okay so what does this mean?\nIt means, BEFORE WE SEE THE DATA we are comfortable with different regression lines."
  },
  {
    "objectID": "10-bayes.html#okay-so-why-is-this-important",
    "href": "10-bayes.html#okay-so-why-is-this-important",
    "title": "Bayes",
    "section": "Okay so why is this important?",
    "text": "Okay so why is this important?\n\nA model that makes impossible predictions prior to seeing the data isn‚Äôt too useful. Why waste the effort? We often know what values are likely, given what we know about effect sizes\nThis is exactly what we do with standard ‚Äúfrequentist‚Äù methods. They have implicit priors such that all values, from negative infinity to positive infinity are equally likely.\nIf we use priors from a uniform distribution we will get the EXACT same results as a frequentist method."
  },
  {
    "objectID": "10-bayes.html#tying-it-together",
    "href": "10-bayes.html#tying-it-together",
    "title": "Bayes",
    "section": "Tying it together",
    "text": "Tying it together\n\nBe comfortable with a different estimation algorithm\nThink of results in terms of distributions\nBe comfortable integrating prior knowledge\n\n\\[p(\\theta | data) \\propto \\frac{p(data | \\theta) \\times p(\\theta )}{p(data)}\\] P(Œ∏|data) is the posterior probability.\nP(Œ∏) is the prior probability.\np(data| \\(\\theta\\) ) is the likelihood.\np(data) can be ignored, it is just a normalized coefficient"
  },
  {
    "objectID": "10-bayes.html#combining-the-three-components",
    "href": "10-bayes.html#combining-the-three-components",
    "title": "Bayes",
    "section": "Combining the three components",
    "text": "Combining the three components\nPriors influencing Posterior"
  },
  {
    "objectID": "10-bayes.html#sample-size-influence",
    "href": "10-bayes.html#sample-size-influence",
    "title": "Bayes",
    "section": "sample size influence",
    "text": "sample size influence"
  },
  {
    "objectID": "10-bayes.html#going-from-prior-to-posterior",
    "href": "10-bayes.html#going-from-prior-to-posterior",
    "title": "Bayes",
    "section": "Going from prior to posterior",
    "text": "Going from prior to posterior\nWhat is our regression estimate again?\n\n\nCode\nfit.1.bayesian %&gt;% \nspread_draws(b_Intercept, b_parent) %&gt;% \nselect(b_parent) %&gt;% \n  mode_hdi(.width = c(.95))\n\n\n# A tibble: 1 √ó 6\n  b_parent .lower .upper .width .point .interval\n     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    \n1    0.640  0.564  0.722   0.95 mode   hdi"
  },
  {
    "objectID": "10-bayes.html#going-from-prior-to-posterior-1",
    "href": "10-bayes.html#going-from-prior-to-posterior-1",
    "title": "Bayes",
    "section": "Going from prior to posterior",
    "text": "Going from prior to posterior\nWith a prior for b0 of N(0, .5)\n\n\nCode\nfit.1.bayesian %&gt;% \nspread_draws(b_Intercept, b_parent) %&gt;% \n  ggplot(aes(x = b_parent)) +\n  stat_slab() +\n  stat_function(data = data.frame(x = c(-2, 2)), aes(x), fun = dnorm, n = 100, args = list(0, .5))"
  },
  {
    "objectID": "10-bayes.html#going-from-prior-to-posterior-2",
    "href": "10-bayes.html#going-from-prior-to-posterior-2",
    "title": "Bayes",
    "section": "Going from prior to posterior",
    "text": "Going from prior to posterior\n\nplusible lines prior to data ‚Äì&gt; plausible lines after data"
  },
  {
    "objectID": "10-bayes.html#what-is-confusing",
    "href": "10-bayes.html#what-is-confusing",
    "title": "Bayes",
    "section": "What is confusing:",
    "text": "What is confusing:\n\n\nIs it a philosophical different frame work? We can talk about how it is p(H0|d) vs p(d|H0) but it really doesn‚Äôt matter. Become a Bayesian just means using the algorithm, and again, most of us don‚Äôt have strong algo preferences\nTechnically we don‚Äôt have P-values, but Bayesian has analogues. Technically there isn‚Äôt NHST (because no null distribution to create sampling distribution) but you can easily do it.\n\nWhy don‚Äôt we do this already? Isn‚Äôt frequentist better? Historical accident due to computation limitations\nBayes Factors. Mostly garbage (imho) as they can be easily manipulated. But they have their place. BFs =/= Bayesian."
  },
  {
    "objectID": "lectures.html",
    "href": "lectures.html",
    "title": "Lectures",
    "section": "",
    "text": "What are models?\nGLM basics\nGLM basics - 2\nGLM basics - 3\nInteractions\nInteractions - 2\nMLM\nSEM\nlogistic\nBayes\nMachine learning 1\nMachine learning 2"
  },
  {
    "objectID": "homework.html",
    "href": "homework.html",
    "title": "homework",
    "section": "",
    "text": "Homework 9\nHomework 8\nHomework 7\nHomework 6\nHomework 5\nHomework 4\nHomework 3\nHomework 2\nHomework 1"
  },
  {
    "objectID": "1-HW.html",
    "href": "1-HW.html",
    "title": "Homework 1",
    "section": "",
    "text": "We will use the `palmerpenguins` dataset. Please install the pacakge `palmerpenguins` if you do not already have it and use the `penguins` data.frame.\nRemember that if you work with a peer or use AI tools like Chat-GPT, you must cite them in your code.\nYou will submit both your .Rmd and HTML files to Canvas. For all figures, you must be intentional. Change the axes names, the theme etc. Do not settle for the defaults!\nGood luck!"
  },
  {
    "objectID": "1-HW.html#question-1",
    "href": "1-HW.html#question-1",
    "title": "Homework 1",
    "section": "Question 1",
    "text": "Question 1\nRun a model to test the question:\n&gt; Do male and female penguins have different length flippers?\n\nYes or no, is the flipper length of male penguins signiciantly different from the flipper length of female penguins? How do you know?\nWhat is the (mean) flipper length for males and females, respectively?\nIs this a good model? How do you know?\nUsing the output, write the formal equation (see Slide 26 if confused)\nInterpret the intercept and the regression coefficient\nIf you were back in your OG Psych Stats days, what would we call this statistical test?\nMake a figure to illustrate this relationship. In it, you should include a summary metric (e.g., the mean or median) as well as information about the distribution of data, the raw data, or both. Look at slides for inspiration!"
  },
  {
    "objectID": "1-HW.html#question-2",
    "href": "1-HW.html#question-2",
    "title": "Homework 1",
    "section": "Question 2",
    "text": "Question 2\nRun a model to test the question:\n&gt; Are penguins on different islands the same size?\n\nTry to write the formal equation -- just do your best! In a sentence or two, what makes this equation different from the one you wrote in Question 1?\nIf you were back in your OG Psych Stats days, what would we call this statistical test?\nGiven the name of this statistical test, replace `lm` with a different function. That is, use a function that is *not* `lm` to run the same model. (I‚Äôm being intentionally vague here‚Ä¶). What changes about this new output, compared to the model in Question 1, and what stays the same?\nMake a figure to compare the size of penguins on different islands. Make sure to give an interval around the point estimate (*hint: this usually comes in the form of ‚Äòbars‚Äô*)\nIs this a good model? How do you know?\nGiven the name of this statistical test, calculate an effect size. What do you notice between this effect size and the `lm` object output? Any similarities?"
  },
  {
    "objectID": "1-intro.html#glm",
    "href": "1-intro.html#glm",
    "title": "What are models?",
    "section": "GLM",
    "text": "GLM\n\nGeneral(ized) Linear Model\nA workhorse that is responsible for &gt;99% of statistical tests in psychology, as well as the building block of many machine learning models"
  },
  {
    "objectID": "1-intro.html#what-do-we-mean-by-generalized",
    "href": "1-intro.html#what-do-we-mean-by-generalized",
    "title": "What are models?",
    "section": "What do we mean by General(ized)?",
    "text": "What do we mean by General(ized)?\n\nIt is general in that it refers to a broad set of similar models that can applied to almost any context"
  },
  {
    "objectID": "1-intro.html#what-do-we-mean-by-linear",
    "href": "1-intro.html#what-do-we-mean-by-linear",
    "title": "What are models?",
    "section": "What do we mean by linear?",
    "text": "What do we mean by linear?\n\nWe try to understand our dependent variable (DV) via a linear combination predictor variables.\nA linear combination a way of combining things (variables) using scalar multiplication and addition"
  },
  {
    "objectID": "1-intro.html#what-is-a-model",
    "href": "1-intro.html#what-is-a-model",
    "title": "What are models?",
    "section": "What is a model?",
    "text": "What is a model?"
  },
  {
    "objectID": "1-intro.html#what-is-a-model-1",
    "href": "1-intro.html#what-is-a-model-1",
    "title": "What are models?",
    "section": "What is a model?",
    "text": "What is a model?\n\na representation of the world\na statistical model uses math to make predictions about the world"
  },
  {
    "objectID": "1-intro.html#middle-school-math",
    "href": "1-intro.html#middle-school-math",
    "title": "What are models?",
    "section": "Middle School Math",
    "text": "Middle School Math\n\\[ y = mx + b \\] - what is \\(y\\)?\n\nwhat is \\(m\\)?\nwhat is \\(x\\)?\nwhat is \\(b\\)?"
  },
  {
    "objectID": "1-intro.html#lets-rewrite-this",
    "href": "1-intro.html#lets-rewrite-this",
    "title": "What are models?",
    "section": "Let‚Äôs rewrite this",
    "text": "Let‚Äôs rewrite this\n\\[y = b_0 + b_{1}X\\]\n\nwhat is \\(y\\)?\nwhat is \\(b_0\\)?\nwhat is \\(b_1\\)?\nwhat is \\(X\\)?"
  },
  {
    "objectID": "1-intro.html#are-models-always-right",
    "href": "1-intro.html#are-models-always-right",
    "title": "What are models?",
    "section": "Are models always right?",
    "text": "Are models always right?"
  },
  {
    "objectID": "1-intro.html#models-are-flawed",
    "href": "1-intro.html#models-are-flawed",
    "title": "What are models?",
    "section": "MODELS ARE FLAWED",
    "text": "MODELS ARE FLAWED\n\nHow do we compensate?"
  },
  {
    "objectID": "1-intro.html#models-are-flawed-1",
    "href": "1-intro.html#models-are-flawed-1",
    "title": "What are models?",
    "section": "MODELS ARE FLAWED",
    "text": "MODELS ARE FLAWED\n\nHow do we compensate?\n\n\\(y = b_0 + b_{1}X + e\\)"
  },
  {
    "objectID": "1-intro.html#models",
    "href": "1-intro.html#models",
    "title": "What are models?",
    "section": "Models",
    "text": "Models\n\nWhat are the goals of modeling?\nWhat do you need in order to develop a model?"
  },
  {
    "objectID": "1-intro.html#how-do-we-know-if-a-model-is-good",
    "href": "1-intro.html#how-do-we-know-if-a-model-is-good",
    "title": "What are models?",
    "section": "How do we know if a model is good?",
    "text": "How do we know if a model is good?\n\nWhat makes it good?"
  },
  {
    "objectID": "1-intro.html#how-will-we-use-models",
    "href": "1-intro.html#how-will-we-use-models",
    "title": "What are models?",
    "section": "How will we use models?",
    "text": "How will we use models?\n\nThis semester, we will mainly focus on classic statistical tests\nEvery single one of these is a model\nWe will also focus on developing your intuition\nWhen you face new models, come back to these basics"
  }
]