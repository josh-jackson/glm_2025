---
title: 5067 Bayes
format: revealjs
slide-number: true
editor: source
execute:
  echo: true
html:
  code-fold: true
  code-summary: Show the code
---

### Reassurance, before we get to the three steps

-   Not drastically different!
-   You get to keep everything you like
-   Your models stay the same!!

## A working mental model

What are Bayesian models?

1.  "Standard" regression with a different estimation algorithm.

2.  Results that represent a distribution rather than a point estimate and some uncertainty.

3.  Priors that incorporate existing knowledge.

## Step 1 is easy

::: incremental
-   OLS i.e. $min\sum(Y_{i}-\hat{Y})^{2}$
-   R uses QR decomposition, Newton Raphson, Fisher Scoring, SVR, etc -- not this equation.
-   More advanced stats use an even different algorithm (e.g., maximum likelihood)
-   So why use bayesian algo? Always works. Fewer assumptions. Lots o' tricks: Regularization in model, imputation in model, etc.
:::

## 2. Think of results in terms of distributions

::: incremental
-   Results are typically thought of as an estimate and an SE
-   Indicates a "best guess" ie mean/median/mode and the imprecision related to it
-   If this guess is far away from zero (and imprecision not large), then it is "significant"
-   We know that if we repeated this again we won't get the same answer (estimate), but likely in between our CIs
:::

## First bayes example

```{r}
#| code-fold: true

library(tidyverse)
galton.data <- psychTools::galton
```

```{r, echo = FALSE}
galton.data %>% 
  ggplot(aes(x = parent, y = child)) +
  geom_jitter(alpha = 1/2) 

```


## regression with brms

```{r}
#| code-fold: true
library(brms)
fit.1.bayesian <- brm(child ~ parent, data = galton.data,
                      backend = "cmdstanr",
                      file = "fit.1.b")
```

```{r}
summary(fit.1.bayesian)
```

## what do these estimates mean?

```{r}
#| code-fold: true
library(tidybayes)
fit.1.bayesian %>% 
spread_draws(b_Intercept, b_parent) %>% 
ggplot(aes(x = b_parent)) +
  stat_halfeye()
```

## Posterior distribution (ie results)

-   Is made up of a series of educated guesses (via our algorithm), each of which is consistent with the data.

-   Greater density, greater likelihood of our parameter.

-   Unlike CI which every value is equally plausible until it is implausible because it is out of the CI. Not just a best guess and an SE around the best guess (as with Maximum Likelihood).

-   In standard practice we can assume this distribution (typically normal), but with bayes it can be flexible!

## Posterior distribution (ie results)

Is made of up of a series of educated guesses. Each dot represents a particular guess. Guesses that occur more often are considered more likely.

```{r}
#| code-fold: true
fit.1.bayesian %>% 
spread_draws(b_Intercept, b_parent) %>% 
ggplot(aes(x = b_parent)) +
  stat_dotsinterval()
```

## Posterior distribution (ie results)

```{r}
#| code-fold: true
fit.1.bayesian %>% 
spread_draws(b_Intercept, b_parent)
```

## How does the algorithm work?

-   Bayes formula is analytically intractable (due to complex differential equations) so one needs to use an algorithm

-   Played a role in developing the thermonuclear bomb with one of the earliest computers. Published in 1953 but ignored within stats b/c it was published within a physics/chemistry journal. Took about until 1990 for desktop computers to run fast enough to do at home.


-----------

1. Choose starting value, evaluate log likelihood
2. Choose proposal by adding subtracting noise
3. Estimate proposal log likelihood
4. Compare starting value with proposal LLs
5. If ratio > 1, keep
6. If ratio < 1, compare with runif(1,min=0,max=1)
7. If ratio is larger keep, otherwise discard
8. Repeate for new proposals



## Bayes is just counting

:::: {.columns}
::: {.column width="30%"}

-   What p is consistent with our data? The most consistent are more likely, but the less consistent are possible

:::

::: {.column width="70%"}
```{r}
#| code-fold: true
p <- seq(0.01, 0.99, by = 0.01)
loglike <- dbinom(2, size = 10, prob = p)
as.tibble(loglike) %>% 
  mutate(p = row_number()/100) %>% 
ggplot(aes(y = value, x=p)) + 
     geom_line() + ylab("likelihood")

```
:::

::::


------------------------------------------------------------------------

Our posterior is literally made up of educated parameter guesses by the algorithm. Which estimates are more likely given the data?

```{r}

tidy_draws(fit.1.bayesian)
```

## Bayesian analysis is just counting

-   Bayesian analysis counts all ways that something can happen (according to assumptions/model). Assumptions with more ways that are consistent with data are more plausible.

-   This method is not demonstrably different than standard approaches. Standard likelihood approaches use the values that are most consistent with the data as an estimate. Try out all possible numbers and then tells you which *one* is most likely.

-   Where Bayes differs, is that there is no ONE result, that there are many possible results that are consistent with the data

## 3. Be comfortable integrating prior knowledge

-   Priors insert knowledge you have outside of your data into your model

-   This can seem "subjective" as opposed to the more "objective" way of letting the data speak.

::: incremental
-   We will mostly not "tip the scales" towards an outcome we want.
-   Most of the time the prior knowledge constrains plausible or implausible *range* of values e.g. we know an effect size of a million is very unlikely.
-   Often priors don't matter...
:::

------------------------------------------------------------------------


$$ child_i \sim Normal( \mu_i , \sigma )\ $$

$$\mu_i = \beta_0 + \beta_1  parent_i $$

-   We need to put priors on each parameter we want to estimate, here $b_{0}$ & $b_{1}$ (and e).

-   $b_{0}$ is the intercept and reflects average child height when parent height is centered.

-   We know, roughly, what average height of adults are so we can create a distribution, say \~N(66 (5.5 ft), 5). That means we are pretty sure (95%) the average height is between \~4'8 and 6\`4

------------------------------------------------------------------------

```{r}
#| code-fold: true
p.0 <-
  tibble(x = seq(from = 40, to = 100, by = .1)) %>% 
  
  ggplot(aes(x = x, y = dnorm(x, mean = 66, sd = 5))) +
  geom_line() +
  scale_x_continuous(breaks = seq(from = 40, to = 100, by = 10)) +
  labs(title = "mu ~ dnorm(66, 5)",
       y = "density")

p.0
```

------------------------------------------------------------------------

-   We could argue that the $b_{1}$ parameter (which indexes the strength of association between parent and child height) is positive. But we don't want to stack the deck.

-   Let's center it around zero, saying that the most plausible estimate is no association, but that we are willing to entertain some strong effects in either direction.

-   In sum: we tend not to make guesses on the average parameter estimate, but instead constraints on the range of possible values. Constraints tell us how "surprised" we'd be if we get some estimate

------------------------------------------------------------------------

```{r}
#| code-fold: true

N10 <- ggplot(data.frame(x = c(-35, 35)), aes(x)) +
  stat_function(fun = dnorm, n = 100, args = list(0, 10))+
  labs(title = "B ~ Normal(0, 10)")

N5 <- ggplot(data.frame(x = c(-35, 35)), aes(x)) +
  stat_function(fun = dnorm, n = 100, args = list(0, 5)) +
  labs(title = "B ~ Normal(0, 5)")

N1 <- ggplot(data.frame(x = c(-35, 35)), aes(x)) +
  stat_function(fun = dnorm, n = 100, args = list(0, 1)) +
  labs(title = " B ~ Normal(0, 1)")

N2 <- ggplot(data.frame(x = c(-35, 35)), aes(x)) +
  stat_function(fun = dnorm, n = 100, args = list(0, 2)) +
  labs(title = "B ~ Normal(0, 2)")

library(patchwork)

(N10 | N5 )/
  (N1 | N2 )
```

## Okay so what does this mean?

It means, *BEFORE WE SEE THE DATA* we are comfortable with different regression lines. For N(0,5) this is what we would expect:

```{r}
#| code-fold: true

pp <-
  tibble(n = 1:100,
         a = rnorm(100, mean = 68, sd = 5),
         b = rnorm(100, mean = 0,   sd = 5)) %>% 
  expand(nesting(n, a, b), height = range(galton.data$parent)) %>%   
  mutate(c.height = a + b * (height - mean(galton.data$parent))) 

g.pp <- pp %>% 
  ggplot(aes(x = height, y = c.height, group = n)) +
  geom_line(alpha = 1/10) +
  coord_cartesian(ylim = c(36, 96)) 

g.pp


```

## Okay so why is this important?

-   A model that makes impossible predictions prior to seeing the data isn't too useful. Why waste the effort? We often know what values are likely, given what we know about effect sizes

-   We entertain impossible values with standard "frequentist" methods. They have implicit priors such that all values, from negative infinity to positive infinity are equally likely.

-   If we use priors from a uniform distribution we will get the EXACT same results as a frequentist method.

## model

$$ child_i \sim Normal( \mu_i , \sigma )\ $$

$$\mu_i = \beta_0 + \beta_1  (parent_i - \bar{parent_i} ) $$
$$\beta_0 \sim Normal(66, 5)\ $$ 
$$\beta_1 \sim Normal(0, 5)\ $$ 
$$\sigma \sim HalfCauchy(0,10) $$

## Tying it together

1.  Be comfortable with a different estimation algorithm
2.  Think of results in terms of distributions
3.  Be comfortable integrating prior knowledge

## Tying it together

$$p(\theta | data) \propto \frac{p(data | \theta) \times p(\theta )}{p(data)}$$ P(θ\|data) is the posterior probability.

P(θ) is the prior probability.

p(data\| $\theta$ ) is the likelihood (really no different than ML).

p(data) can be ignored, it is just a normalized coefficient

## Tying it together

-   p(data \| $\theta$) is what we typically work with. What is the definition of a p-value?

-   We (scientists and those wanting to make decisions based on the science) don't want that, but rather they want p($\theta$ \| data). Inverse probability - working backwards from the data to determine the most likely underlying condition

-   Bayes gives us this.

## Combining the three components

Priors influencing Posterior

```{r, message = FALSE, echo = FALSE}
library(gridExtra)
library(tidyverse)
sequence_length <- 1e3

d <-
  tibble(probability = seq(from = 0, to = 1, length.out = sequence_length)) %>% 
  tidyr::expand(probability, row = c("flat", "stepped", "Laplace")) %>% 
  arrange(row, probability) %>% 
  mutate(prior = ifelse(row == "flat", 1,
                        ifelse(row == "stepped", rep(0:1, each = sequence_length / 2),
                               exp(-abs(probability - .5) / .25) / ( 2 * .25))),
         likelihood = dbinom(x = 6, size = 9, prob = probability)) %>% 
  group_by(row) %>% 
  mutate(posterior = prior * likelihood / sum(prior * likelihood)) %>% 
  gather(key, value, -probability, -row) %>% 
  ungroup() %>% 
  mutate(key = factor(key, levels = c("prior", "likelihood", "posterior")),
         row = factor(row, levels = c("flat", "stepped", "Laplace"))) 

p1 <-
  d %>%
  filter(key == "prior") %>% 
  ggplot(aes(x = probability, y = value)) +
  geom_line() +
  scale_x_continuous(NULL, breaks = c(0, .5, 1)) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "prior") +
  theme(panel.grid       = element_blank(),
        strip.background = element_blank(),
        strip.text       = element_blank()) +
  facet_wrap(row ~ ., scales = "free_y", ncol = 1)

p2 <-
  d %>%
  filter(key == "likelihood") %>% 
  ggplot(aes(x = probability, y = value)) +
  geom_line() +
  scale_x_continuous(NULL, breaks = c(0, .5, 1)) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "likelihood") +
  theme(panel.grid       = element_blank(),
        strip.background = element_blank(),
        strip.text       = element_blank()) +
  facet_wrap(row ~ ., scales = "free_y", ncol = 1)

p3 <-
  d %>%
  filter(key == "posterior") %>% 
  ggplot(aes(x = probability, y = value)) +
  geom_line() +
  scale_x_continuous(NULL, breaks = c(0, .5, 1)) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "posterior") +
  theme(panel.grid       = element_blank(),
        strip.background = element_blank(),
        strip.text       = element_blank()) +
  facet_wrap(row ~ ., scales = "free_y", ncol = 1)

library(gridExtra)

grid.arrange(p1, p2, p3, ncol = 3)
```

## sample size influence

```{r, echo = FALSE}

bernoulli_likelihood <- function(theta, data) {
  # `theta` = success probability parameter ranging from 0 to 1
  # `data` = the vector of data (i.e., a series of 0s and 1s)
  n   <- length(data)
  return(theta^sum(data) * (1 - theta)^(n - sum(data)))
}
  
small_data <- rep(0:1, times = c(3, 1))

s <- tibble(theta =   seq(from = 0,     to = 1, by = .001),
       Prior = c(seq(from = 0,     to = 1, length.out = 501),
                 seq(from = 0.998, to = 0, length.out = 500))) %>% 
  mutate(Prior      = Prior / sum(Prior),
         Likelihood = bernoulli_likelihood(theta = theta,
                                           data  = small_data)) %>% 
  mutate(marginal_likelihood = sum(Prior * Likelihood)) %>% 
  mutate(Posterior           = (Prior * Likelihood) / marginal_likelihood) %>% 
  select(theta, Prior, Likelihood, Posterior) %>% 
  gather(key, value, -theta) %>% 
  mutate(key = factor(key, levels = c("Prior", "Likelihood", "Posterior")))  

small <- ggplot(s, aes(x = theta, ymin = 0, ymax = value)) +
  geom_ribbon(fill = "grey67") +
  scale_x_continuous(breaks = seq(from = 0, to = 1, by = .2)) +
  labs(x = expression(theta),
       y = "probability density") +
  theme(panel.grid = element_blank()) +
  facet_wrap(~key, scales = "free_y", ncol = 1)


large_data <- rep(0:1, times = c(30, 10))

l <- tibble(theta =   seq(from = 0,     to = 1, by = .001),
       Prior = c(seq(from = 0,     to = 1, length.out = 501),
                 seq(from = 0.998, to = 0, length.out = 500))) %>% 
  mutate(Prior      = Prior / sum(Prior),
         Likelihood = bernoulli_likelihood(theta = theta,
                                           data  = large_data)) %>% 
  mutate(marginal_likelihood = sum(Prior * Likelihood)) %>% 
  mutate(Posterior           = (Prior * Likelihood) / marginal_likelihood) %>% 
  select(theta, Prior, Likelihood, Posterior) %>% 
  gather(key, value, -theta) %>% 
  mutate(key = factor(key, levels = c("Prior", "Likelihood", "Posterior"))) 
  
 large <- ggplot(l, aes(x = theta, ymin = 0, ymax = value)) +
  geom_ribbon(fill = "grey67") +
  scale_x_continuous(breaks = seq(from = 0, to = 1, by = .2)) +
  labs(x = expression(theta),
       y = "probability density") +
  theme(panel.grid = element_blank()) +
  facet_wrap(~key, scales = "free_y", ncol = 1)
library(patchwork)
(small | large)
```

## Going from prior to posterior

What is our regression estimate again?

```{r}
#| code-fold: true
fit.1.bayesian %>% 
spread_draws(b_Intercept, b_parent) %>% 
select(b_parent) %>% 
  mode_hdi(.width = c(.95))
```

## Going from prior to posterior

With a prior for b0 of N(0, .5)

```{r}
#| code-fold: true
fit.1.bayesian %>% 
spread_draws(b_Intercept, b_parent) %>% 
  ggplot(aes(x = b_parent)) +
  stat_slab() +
  stat_function(data = data.frame(x = c(-2, 2)), aes(x), fun = dnorm, n = 100, args = list(0, .5)) 
```

## Going from prior to posterior

-   plusible lines prior to data --\> plausible lines after data

```{r, echo = FALSE}
library(modelr)
pp <-
  tibble(n = 1:100,
         a = rnorm(100, mean = 68, sd = 5),
         b = rnorm(100, mean = 0,   sd = .5)) %>% 
  expand(nesting(n, a, b), height = range(galton.data$parent)) %>%   
  mutate(c.height = a + b * (height - mean(galton.data$parent))) 

g.pp <- pp %>% 
  ggplot(aes(x = height, y = c.height, group = n)) +
  geom_line(alpha = 3/10) +
  coord_cartesian(ylim = c(48, 89)) +
  xlab("parent_height")



g.pp2<- galton.data %>% 
data_grid(parent = seq_range(parent, n = 101)) %>% 
 add_epred_draws(fit.1.bayesian, ndraws = 200) %>% 
  ggplot(aes(x = parent, y = child)) +
  geom_line(aes(y = .epred, group = .draw), alpha = .05) +
  geom_point(data = galton.data, size = 1) +
  xlab("parent_height")


(g.pp | g.pp2)
```

## What is confusing:

::: incremental
-   Is it a philosophically different framework? p( ${H_0}$ \| D) vs p(D \| ${H_0}$ ) but it doesn't really matter. Bayesian just means using the algorithm, and again, most of us don't have strong algo preferences. Assuming a flat prior basically makes p( ${H_0}$ \|d) = p( D \| ${H_0}$ ) so if you want (don't tell Bayesians) you can interpret frequentist stats as Bayesian. And that means CIs too! (don't tell frequentists)

-   Why don't we do this already? Isn't frequentist better? History doesn't progress due to fair and even fights (i.e. the correct doesn't always prevail). Computation limitations and Fisher held progress back.
:::

## What is confusing:

::: incremental
-   Technically, we don't have p-values, but Bayesian has analogues.

-   Technically, there isn't NHST (because no null distribution to create sampling distribution) but you can easily do "it". Remember how model comparisons are equivalent to typical NHST tests?

-   Bayes Factors. Mostly garbage (imho) as they can be easily manipulated. But they have their place. BFs =/= Bayesian.
:::
